\documentclass[12pt]{article}


% \usepackage[authoryear]{natbib}
% \usepackage{amsmath}
% \usepackage{hyperref}
% \usepackage{hyperref}
% \usepackage{geometry}
% \usepackage{graphicx}
% \usepackage{amsfonts}

\input{../../paperProduction/occBind/docs/AMArepresentationNewCmds}



\author{Gary S. Anderson\thanks{The analysis and conclusions set forth are those of the author and do not indicate concurrence by other members of the research staff or the Board of Governors. I would like to thank Luca Guerrieri, Christopher Gust, Hess Chung, Benjamin Johannsen  and Robert Tetlow for their comments and suggestions.  Special thanks to Luca Guerieri for first noticing that the series representation formulation could lead to an error bound for model solutions.}}

\title{A New Series Representation for 
Nonlinear Dynamic Stochastic Model Solutions: General Error Bounds and New Solution Algorithm}

\date{\today: \currenttime}
\begin{document}

\maketitle

\begin{abstract}
This paper proposes a new series representation for bounded time series paths and uses this representation to construct error bounds for proposed solutions to a very general class of dynamic stochastic models including models subject to occasionally binding constraints and/or regime switching.
This error bound calculation which, incorporating the impact of errors across  all the equations in 
the system makes, it possible to compute, ahead of time, estimates of the computational cost for a
given level of accuracy of each component of the solution.
These error bounds should prove useful for assessing the accuracy of any
proposed model solution regardless of the source.


In addition, the paper also proposes a new algorithm for solving these models.
The series representation makes it possible to reliably improve upon an initial
guess for a stochastic model solution by solving a 
potentially complicated, but deterministic problem in the initial time period.
Like the error bound formula, the algorithm is appropriate for models with occasionally binding constraints and/or regime switching. 


The  paper uses a particular implementation of the algorithm to
demonstrate how to use the 
series representation in conjunction with 
Smolyak polynomial function approximation to avoid repeated numerical integration
and to exploit the high degree of parallelism available in the algorithm.







\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}





Stochastic dynamic non linear economic
models increasingly embody  occasionally binding constraints (OBC).
Since \cite{Christiano2000} a host of
authors have described a variety of approaches. 
\cite{holden15:_exist_dsge,guerrieri15:_occbin,benigno09,hintermaier10,brumm10,nakov08,haefke98,nakata12,gordon11,billi11,Hintermaier2010,Guerrieri2015}
This paper provides yet another.  This new series representation provides  a coherent framework for attacking a wide variety of complicated nonlinear models.

The analysis develops a new series representation for bounded time series.  The author has found no comparable use of a reference dynamical system to generate a transformation of one infinite dimensional series into another. 

The framework provides a new way to bound the error one can expect from
employing a given proposed model solution and leads to a
algorithm with  components similar to parameterized expectations that
one can use to improve proposed solutions. The series representation makes
it possible to organize the calculation around computing a deterministic
problem at time t given a proposed solution.  The deterministic solution
can accommodate inequality constraints or alternative regimes to produce a
solution for each set of initial conditions.  One can typically arrange,
perhaps by adding auxiliary variables, to produce a ``decision rule''
that one can use to correctly precompute a deterministic conditional
expectation function that can be iterated forward and serves to
improve upon the original proposed solution.
Time invariant stochastic functions 
lead naturally to an associated family of deterministic maps
which can conveniently represented by the series representation.


Section \ref{sec:newseries} presents a useful new series representation for any bounded time series.
Section \ref{sec:extToMaps} shows how to apply this series representation to time invariant maps.
Section \ref{sec:solnerrorbounds} provides formulas for computing dynamic stochastic model error bounds for proposed  solutions.
Section \ref{sec:algoforsoln} presents a new solution algorithm.
Section \ref{sec:future} discusses directions for future work.
Section \ref{sec:conc} concludes.

\section{A New Series Representation For  Bounded Time Series}
\label{sec:newseries}

\subsection{Linear Reference Models and a Formula for  ``Anticipated Shocks''}
\label{sec:linref}




For any linear homogeneous 
$L$ dimensional 
deterministic 
system 
\begin{gather}
  	 H_{-1} x_{t-1} + H_0 x_t + H_1 x_{t+1}=0\label{hSystem}
\end{gather}
that produces  a unique stable solution, 
it is well known\ \citep{anderson10} that  inhomogeneous solutions 
\begin{gather}
	 H_{-1} x_{t-1} + H_0 x_t + H_1 x_{t+1}=\psi_\epsilon \epsilon +\psi_{c}
\intertext{ can be computed as}
x_t=B x_{t-1} + \phi \psi_\epsilon \epsilon + (I - F)^{-1} \phi \psi_c
\intertext{where}
\phi= (H_0 +H_1 B)^{-1}  \text{ and } \,\,F=-\phi H_1 
\end{gather}
It will be useful to collect the components of this representation for use in
subsequent sections of the paper.
Define $\linMod \equiv \linModMats$.

Consider an arbitrary, but bounded path
 \begin{gather}
   \xWOarg \in{R^L}\,\text{ with }\,\infNorm{\xWOarg}  \le \bar{\mathcal{X}}\,\,\,\,\,\forall t> 0 \label{aBPath}.
 \end{gather}

{\small
Now, given the trajectories \refeq{aBPath}, define 
$  z_{t}$ as  
\begin{gather}
  z_{t} \equiv H_{-1} \mathcal{X}_{t-1} +  H_0 \mathcal{X}_{t} +  H_1 \mathcal{X}_{t+1} \label{defZ} 
\end{gather}




\begin{theorem}
	 \begin{gather}
	 \mathcal{X}_{t} =B x_{t-1}+ \phi \psi_\epsilon\epsilon + (I - F)^{-1} \phi \psi_c + \sum_{\sForSum=0}^\infty F^s \phi z_{t+\sForSum} \label{theSeries}
\intertext{and}
	 \mathcal{X}_{t+1} =B \mathcal{X}_{t}  + (I - F)^{-1} \phi \psi_c+ \sum_{\sForSum =0}^\infty F^\sForSum \phi z_{t+\sForSum+1} \,\,\,\,\,\forall t \ge  0.
	 \end{gather}
\end{theorem}
}
\begin{myProof}
See \citep{anderson10},
\end{myProof}


	 Consequently, given a bounded time series \refeq{aBPath},
and a stable linear homogeneous system like \refeq{hSystem},
one can easily compute a corresponding series representation 
\refeq{theSeries}.
Interestingly, the linear model, $H$, the  constant term $\psi_c$ and the
impact of the stochastic shocks $\psi_\epsilon $ can  be 
chosen rather arbitrarily -- the only constraint being the existence of a saddle-point solution or the linear system.  Under certain rank conditions this series representation constitutes a one to one onto transformation between the
$L$ dimensional $x_t$ and $z_t$ time series.
The formula will provide a series  for any 
$L$ dimensional $\linMod$.
Later,this observation will give us some confidence in the 
robustness of the algorithms described in section 
\ref{sec:unknown-solutions} for constructing series 
representations for unknown families of functions 
satisfying complicated systems of dynamic non-linear equations.


A key feature to note is that the series representation can accommodate arbitrarily complex time series trajectories, so long as these trajectories are bounded.

Under certain rank conditions, this transformation is one to one and onto.  The
transformation $ {\{ x_{t}, x_{t}, x_{t+1},x_{t+2}\ldots\}} \rightarrow \linMod(x_{t-1}) \rightarrow{\{ z_{t}, z_{t+1}, z_{t+2},\ldots\}} $ is invertible under
certain rank conditions. $ {\{ z_{t}, z_{t+1}, z_{t+2},\ldots\}} \rightarrow \linMod(x_{t-1})^{-1} \rightarrow{\{ x_{t}, x_{t}, x_{t+1},x_{t+2}\ldots\}} $ and the inverse can be interpreted as giving the impact of ``fully anticipated future shocks'' on the path of $x_t$ obeying the solution of a linear rational expectations model.  The formula supports the intuition that far distant shocks influence current conditions less than  imminent shocks.


\subsection{An Example: An ``Almost'' Arbitrary Linear Model and  Some Arbitrary  Bounded Time Series}
\label{sec:almostarbitrary}


Consider the following constructed from ``almost'' arbitrary coefficients
\begin{gather}
  \begin{bmatrix}
H_{-1}&H_{0}&H_{1} 
  \end{bmatrix}=
\vcenter{\hbox{\includegraphics{refHmat.pdf}}}\intertext{with $\psi_c=\psi_\epsilon=0, \,\,  \psi_z=I$.
These coefficients are not completely arbitrary in so far as the series 
representation requires that the linear model
has a unique stable solution.}
  B=
\vcenter{\hbox{\includegraphics{refBmat.pdf}}}\\
\phi=
\vcenter{\hbox{\includegraphics{refPhimat.pdf}}}\\
F=
\vcenter{\hbox{\includegraphics{refFmat.pdf}}}
\end{gather} 



The existence of a series 
representation requires only that the state values along the 
paths remain bounded.  For example, consider the following three
bounded families of state vector paths:
\begin{gather}
  x_{1,t}=\alpha D_\pi(t) \\
x_{2,t}=\beta (-1)^t\\
x_{3,t}=\epsilon_t 
\end{gather} 
where $D_\pi(t)$ gives the t-th digit of $\pi$ and the $\epsilon_t$ are a sequence of pseudo random draws from the uniform distributions $\mathcal{U}(-4,4)$

The first set of trajectories(See Figure \ref{arbpaths}) is a function of
the digits in the decimal representation of $\pi$.  
determined by a nonlinear function of the initial conditions and a random shock.
The second set of trajectories (See Figure \ref{arbpaths}) oscillates between two values
determined by  a nonlinear function of the initial conditions and the shock.
The third set of trajectories (See Figure \ref{arbpaths}) is a sequence of uniformly distributed random
numbers based on a seed determined by  a nonlinear function of  the initial conditions and the shock.
These examples paths were chosen to emphasize that the trajectories
 need not converge to a fixed point, and 
need not be produced by iteration of a discrete-time map.
The boundedness of the paths is a sufficient condition for the existence 
of the series representation.\footnote{Although useful in some contexts,
this paper will not investigate sufficient conditions for families of
unbounded trajectories.}


\begin{figure}
  \centering
\includegraphics[width=2in]{piPath.pdf}
\includegraphics[width=2in]{oscillPath.pdf}
\includegraphics[width=2in]{pseudoPath.pdf}
\includegraphics[width=3in]{theZs.pdf}  
  
  \caption{Arbitrary Bounded Time Series Paths and Corresponding $z_{i,t}$ values}\label{arbpaths}
\end{figure}


% \begin{figure}
%   \centering
% \includegraphics[width=2in]{oscillPath.pdf}
  
%   \caption{Oscillatory Path}\label{oscillpath}
% \end{figure}

% \begin{figure}
%   \centering
% \includegraphics[width=2in]{pseudoPath.pdf}
%   \caption{Uniformly Distributed Path}\label{pseudopath}
% \end{figure}

% \begin{figure}
%   \centering
% \includegraphics[width=3in]{theZs.pdf}  
%   \caption{The  z's Corresponding to  $x_{-1}=(1,2,3),\epsilon=(2,1,2)$} \label{arbFig}
% \end{figure}

\begin{figure}
  \centering


\includegraphics[width=3in]{arbTruncErr.pdf}  
  \caption{$x_t$ Error Bound Versus Actual Error} \label{figArbTrunc}

\end{figure}


Figure \ref{arbpaths} shows, for a particular initial state vector and shock value,  the paths for the state vectors and the  z's that generate the path.
One can repeat the calculations for any given initial condition to produce
a z series exactly replicating the given set of trajectories.  The family
of z functions along with equation \ref{theSeries} provide a series 
representation for the family of trajectories.  Figure \ref{figArbTrunc} shows
that the truncation error bound is a very conservative measure of the accuracy
of the truncated series.  The series requires only the first 20 terms to compute
the initial value of the state vector to machine precision. 
The series representation can compute the entire series to machine precision
if all the terms are included, but the terms for state vectors closer 
to the initial time have the most important impact.





\subsection{A Path Norm and Assessing $x_t$ Error}
\label{sec:truncationerr}
The formula \ref{theSeries} was derived to compute he impact on the current state of fully anticipated future shocks.  The formula characterizes the impact exactly.  However one can contemplate the impact of at least two types of imprecision.  One could truncate a series of correct values of $z_t$.  One might have imprecise values of $z_t$ along the path.

\subsubsection{Truncation Error}


One could consider approximating $\mathcal{X}_t$ by 
truncating the series \ref{theSeries} at a finite number of terms.
 	 \begin{gather}
 	 \xWOargK \equiv B x_{t-1}+ \phi \psi_\epsilon\epsilon  + (I - F)^{-1} \phi \psi_c + \sum_{s=0}^k F^s \phi z_{t}\label{theTruncSeries}
 \end{gather}
We can bound the  series approximation truncation errors.
Since
    \begin{gather}
      \label{eq:1}
\sum_{s=k+1}^{\infty} F^s \phi \psi_z = (I -F)^{-1} F^{k+1}\phi \psi_z       \\
\infNorm{\xWarg-\xWargK} \le \infNorm{(I -F)^{-1} F^{k+1}\phi \psi_z} \left ( \infNorm{H_{-1} }+ \infNorm{H_{0} }+ \infNorm{H_{1} } \right )\bar{\mathcal{X}}
    \end{gather}
Note that for approximating $\xWarg$ the impact of  a given realization along the path declines for those realizations which are  more temporally distant.



\subsubsection{Path Error}


We can assess the impact of incorrect values for $z_t$ by computing the maximum correction required for the $z_t$ and applying the 
same formula.


\label{sec:pathnorm}

\begin{itemize}
\item MSNTO
\item Almost Arbitrary
\item Impact of Choice
\item represents impact of ``anticipated shocks'' interpretation error bounds on $x_t$
\item any bounded path
\item Should include closed form solution for RBC
\item figure for truncation error incorrect \ref{figArbTrunc}
\item interaction between series  through z's
\end{itemize}


\section{Nonlinear Dynamic Stochastic Time Invariant Maps}
\label{sec:extToMaps}

\begin{figure}
  \centering
  


  \begin{gather}
    \fbox{Nonlinear Rational Expectations Model}\\ \Downarrow\\
\fbox{Bounded Time Invariant Function Solution}\\\Downarrow\\
\fbox{Series Representation}\\\Downarrow\\
\fbox{Series Approximation}
  \end{gather}
  \caption{From Models to Approximate Solutions}
  \label{fig:modelsto}
\end{figure}



\subsection{Application to Time Invariant Maps}


In this paper we will be interested in computing with time
 invariant maps. Many dynamic stochastic models have solutions that fall in this class. 
These time invariant maps generate  bounded time series paths which can
be represented using the framework from section \ref{sec:newseries}.
These time invariant maps impose additional structure on the time 
series they generate.  This structure will allow us to use the series formula to
develop bounds for the error in the solution.
In this section, to keep things simple, we will rule out regime switching and occasionally binding constraints and start
 with  a simple single equation system?
Later, in order to handle models with regime switching and occasionally binding constraints, will need to consider more complicated collections of 
of equation systems with  Boolean gates. We will show how to apply the 
series formulation to get error bounds for these models as well.



\subsubsection{General RBC Example Formulation}
\label{sec:rbcaux}
  See \cite{Maliar2005}
 \begin{gather*}
   \max\left \{  u(c_t^t) + E_t \sum_{\tau=t}^\infty \beta \delta^{\tau+1-t}u(c_{\tau+1}^t)\right \}\\
c_\tau^t + k_\tau^{t+1}=(1-d)k_\tau^{t-1} + \theta_\tau f(k_\tau^{t-1})\\
f(k_\tau^{t-1})= k_\tau^\alpha
 \end{gather*}

\begin{gather}
\frac{1}{c_t^{\eta}}=\alpha \delta k_{t}^{\alpha-1} E_t \left (\frac{\theta_{t}}{c_{t+1}^\eta} \right ) \\
c_t + k_t=\theta_{t-1}k_{t-1}^\alpha \\
 \theta_t =\theta_{t-1}^\rho e^{\epsilon_t}\label{rbcSys}
 \end{gather}


\paragraph{An RBC Model Example Known Solution}
\label{sec:rbcexample}
  {The RBC Model ($\eta=1$): Recovering a Known Solution}

{for $\eta=1$}
 \begin{gather}
\frac{1}{c_t}=\alpha \delta k_{t}^{\alpha-1} E_t \left (\frac{\theta_{t}}{c_{t+1}} \right ) \\
c_t + k_t=\theta_{t-1}k_{t-1}^\alpha \\
\theta_t =\theta_{t-1}^\rho e^{\epsilon_t}\label{rbcSys}
\intertext{and there is a closed form solution}
  k_{t}= \alpha \delta \theta_{t} k_{t-1}^\alpha.\label{soln}\\
c_t=  (1-\alpha \delta) \theta_{t} k_{t-1}^\alpha
\end{gather}
For mean zero iid $\epsilon_t$ we can easily compute a family of bounded trajectories




\begin{gather}
  \begin{bmatrix}
c_{t+s}(k_{t-1},\theta_t,\epsilon_t)\\k_{t+s}(k_{t-1},\theta_t,\epsilon_t)    \\ \theta_{t+s}(\theta_{t-1},\theta_t,\epsilon_t)    
  \end{bmatrix}
\intertext{with conditional mean converging over time to }
  \begin{bmatrix}
    c_{ss}\\k_{ss}
  \end{bmatrix}=
  \begin{bmatrix}
\nu^\alpha-\nu\\ \nu
  \end{bmatrix}\intertext{where}
\nu= \alpha ^{\frac{1}{1-\alpha }} \delta ^{\frac{1}{1-\alpha }}
\end{gather}


We can use the family of conditional expectations
along with the contrived reference model to recover an 
approximation for equation \refeq{soln} along with error bounds.
The series representation provides a weighted sum of z functions that give us
an approximation for the known solution.
\footnote{Note that the reference model is deterministic and the $z$ functions account for the stochastic nature of the model.}
% \footnote{
% We need not  make these adjustments for the steady state,
% but doing so economizes on the number of terms 
% required for a given level of approximation
% accuracy.}

Using the following parameter values

\begin{gather}
\vcenter{\hbox{\includegraphics{../../paperProduction/occBind/docs/RBCParamSubs.pdf}}} \,\, \text{ we have } \,\,
  \begin{bmatrix}
    c_{ss}\\k_{ss} \\ \theta_{ss} \label{rbcparams}
  \end{bmatrix}=
\vcenter{\hbox{\includegraphics{../../paperProduction/occBind/docs/RBCSSVal.pdf}}}
\end{gather}%(\footnote{{../../paperProduction/occBind/docs/RBCParamSubs.pdf}})(\footnote{{RBCSSVal.pdf}})


\begin{figure}
  \centering
\includegraphics{simpBoundsVActual.pdf}  
  \label{rbcTrunc}
  \caption{RBC Truncation Error Bound Versus Actual}
\end{figure}






\subsubsection{Dynamic Stochastic Models, Model Solutions  and Conditional Expectations}
\label{sec:condExp}
\subsection{General Formulation}
\label{sec:genAux}
  {Discovering Unknown Solutions}

A, conceptually, very simple solution strategy:  

  \begin{enumerate}
  \item Begin with some convergent linear model, $\linMod$, of appropriate dimension.
  \item Compute solutions honoring the model equations for time t, 
but that assume the trajectories subsequently 
evolve according to the convergent 
linear model, $\linMod$.
  \item Given solutions valid for time $t$ to $t+k$, extend the solutions to be valid for time $t$ to $t+k+1$ \label{stepNo}
\item Repeat step \ref{stepNo} unless truncation formulas indicate the solution is sufficiently accurate
\item collection of equations systems with logic gate.
  \begin{itemize}
  \item mutually exclusive
  \item exhaustive
  \end{itemize}
  \end{enumerate}










  {Correctly Computing Expectations of Nonlinear Functions}

  \begin{itemize}
\item Auxiliary equations for non linear ``chunks'' provide mechanism for characterizing  nonlinear expectations in the future
  \item Formula maps future equation errors  to current time changes in model
variables.
\item  $\epsilon_t$ is known so that,
given a function characterizing future expectation,  model requires
a deterministic solution at time $t$
\item The formula is linear in the potentially nonlinear $z$ functions
  \end{itemize}







\subsubsection{Auxiliary Variable Reformulation}
\label{sec:aux}

\begin{itemize}
\item iterated expectations $E (x | φ1) = E (E (x | φ2) | φ1) $
  \href{https://stats.stackexchange.com/questions/95947/a-generalization-of-the-law-of-iterated-expectations}{a proof}
\item will want to apply the law of iterated expectations so we will construct auxiliary variables that embed any nonlinear combination  of the original
  variables that might be required to make this work.  Although it is possible to construct models where this may be difficult, many important economic models
  can easily be adapted this way.

\end{itemize}


\begin{itemize}
\item composition in effect imposes for number of periods
\end{itemize}



To describe the algorithm we will, for the moment, omit inequality constraints
and regime switching.
We will develop an algorithm for finding solutions for models that can be written in  the form


\begin{gather}
  h_i(x_{t-1},x_{t},x_{t+1},\epsilon_t)=h^{det}_{io}(x_{t-1},x_{t},\epsilon_t)+\sum_{j=1}^{p_i} [h^{det}_{ij}(x_{t-1},x_{t},\epsilon_t)h^{nondet}_{ij}(x_{t+1})]=0
\end{gather}
This is a very broad class of models including most widely used
macroeconomics models.

For example, the Euler equations for the  neoclassical growth  model 
\label{sec:simple-rbc-model-ext} can be written as
\begin{gather}
h_{10^{det}}(\cdot)=\frac{1}{c_t},\,\,
h_{11}^{det}()=\alpha \delta k_{t}^{\alpha-1} ,\,\,
h_{11}^{nondet}(\cdot)=E_t \left (\frac{\theta_{t+1}}{c_{t+1}} \right )\\
h_{20}^{det}(\cdot)=c_t + k_t-\theta_tk_{t-1}^\alpha,\,\,
h_{21}^{det}(\cdot)=0\\
h_{30}^{det}(\cdot)=\ln \theta_t -(\rho \ln \theta_{t-1} + \epsilon_t),\,\,
h_{31}^{det}(\cdot)=0
\end{gather}
Since we will be working with models where expectations are computed at time t, $\epsilon_t$ is known.  The only stochastic components are those with time subscripts greater than $t$. Since we will need to compute 
the conditional expectation of nonlinear expressions,  
this setup will make it possible for us to use auxiliary
variables to correctly compute the required expected values.
Below, we will consider 
systems that augment these dynamic equations with additional constraints 
on the evolution of the variables.


The algorithm will construct
 a time invariant function $g^\ast$ that satisfies
\begin{gather}
  \begin{split}
h(x_{t+s-1},g^\ast(x_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(x_{t+s-1},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) \label{theProblem} \\
%m(x_{t+s-1},g^\ast(x_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(x_{t+s-1},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) \ge 0  
  \end{split}
%  \intertext{define} 
% \mathcal{G}^\ast(x_{t+s-1},\epsilon_{t+s})= \mathcal{H}[g^\ast(g^\ast(x_{t+s-1},\epsilon_{t+s}),\epsilon_{t+s+1})] \nonumber
 \end{gather}
 for all $s>0$ where $\mathcal{H}$ is an operator, 
  that maps stochastic functions of $x$ and $\epsilon$ into deterministic 
functions of $x$ alone.  In this paper we will consider two such operators:










We begin by choosing some linear model of appropriate dimension that 
has a uniquely convergent steady state.  
Although this is not necessary, it may be possible to obtain this model by
linearizing the original model around the deterministic steady state.\footnote{As noted above, one could construct a series representation using any linear
 model of appropriate dimension with a unique stable solution.  The rate of convergence and the number of terms required for a  given level of accuracy will depend upon the linear model employed.}


 \begin{gather}
 g^0(x_{t-1},\epsilon_{t})=  
B x_{t-1}+ \phi \psi_\epsilon\epsilon_{t} +
 (I - F)^{-1} \phi \psi_c\\ \label{firstIter}
z^{0}_{t+i}(x_{t-1},\epsilon_{t-1})=0 \,\, \forall i \ge 0
 \end{gather}


Needs generalization of ``chunk'' $\frac{1}{c_t}\rightarrow \left (\frac{\theta_{t+1}}{c_{t+1}^\eta} \right )$

  \begin{itemize}
  \item Compute $\linMod$ for RBC Model $\eta=1,\rcpC_t=\frac{1}{c_t}$
  \end{itemize}

We construct our stand-in model by augmenting the model with the equation
\begin{gather}
  \rcpC_t=\frac{1}{c_t}
\end{gather}
substituting $\rcpC_{t+1}$ for $\frac{1}{c_{t+1}}$ in the first equation and 
 linearizing the RBC model about the ergodic mean
given in \refeq{rbcparams}
{\small
\begin{gather}
  \begin{bmatrix}
H_{-1}&H_{0}&H_{1} 
  \end{bmatrix}=\\
\vcenter{\hbox{\includegraphics{RBCHmatSymb.pdf}}} \label{rbcLinSys}
\intertext{with}
\psi_\epsilon=
\begin{bmatrix}
  0\\0\\1\\0
\end{bmatrix}, \psi_z=I
\end{gather}%(\footnote{generated by AMAPaperCalcs.mth {RBCHmatSymb.pdf}})
}
These coefficients  produce a unique stable linear solution.

\begin{gather}
  B=
\vcenter{\hbox{\includegraphics{RBCBmatSymb.pdf}}},
\phi=
\vcenter{\hbox{\includegraphics{RBCPhimatSymb.pdf}}}\\
F=
\vcenter{\hbox{\includegraphics{RBCFmatSymb.pdf}}}\\
\psi_c=
\vcenter{\hbox{\includegraphics{RBCHSum.pdf}}}
\vcenter{\hbox{\includegraphics{RBCSS.pdf}}}=\vcenter{\hbox{\includegraphics{RBCPsissSymb.pdf}}}
\end{gather}%(\footnote{generated by AMAPaperCalcs.mth {RBCBmatSymb.pdf}})(\footnote{generated by AMAPaperCalcs.mth {RBCPhimatSymb.pdf}})(\footnote{generated by AMAPaperCalcs.mth {RBCFmatSymb.pdf}})(\footnote{generated by AMAPaperCalcs.mth {RBCHSum.pdf}})(\footnote{generated by AMAPaperCalcs.mth {RBCSS.pdf}})

Applying the formula \refeq{firstIter} produces:

{\tiny
\begin{gather}
  \begin{bmatrix}
c_t\\k_t\\ \rcpC_t\\\theta_t
  \end{bmatrix}=%paperCalcsRBCExample xt00
   \left(
   \begin{array}{c}
 0.359845 \epsilon _t+0.692632 k_{t-1}+0.341853 \theta _{t-1}-0.0442851
   \text{z1}_{t-1}+0.658 \text{z2}_{t-1}+0.359845 \text{z3}_{t-1}-0.111552 \\
 0.187032 \epsilon _t+0.36 k_{t-1}+0.17768 \theta _{t-1}+0.0442851
   \text{z1}_{t-1}+0.342 \text{z2}_{t-1}+0.187032 \text{z3}_{t-1}-0.0579799 \\
 -5.34898 k_{t-1}+0.342 \text{z1}_{t-1}-5.08153
   \text{z2}_{t-1}+\text{z4}_{t-1}+3.7794 \\
 \epsilon _t+0.95 \theta _{t-1}+\text{z3}_{t-1}+0.05 \\
   \end{array}
   \right)
\end{gather}
}

and 


{\tiny
%xt01=Private`computeNextXt[{Private`bmat,Private`phimat,Private`fmat,Private`psieps,Private`psic,Private`psiz},solnFunc00PF[[3+Range[3]]],{{cc},{kk},{tt}},{1}]//N//Expand//Simplify
\begin{gather}
  \begin{bmatrix}
c_{t+1}\\k_{t+1}\\ \rcpC_{t+1}\\\theta_{t+1}
  \end{bmatrix}=%paperCalcsRBCExample xt00
  \left(
   \begin{array}{c}
 0.471397 \epsilon _t+0.249347 k_{t-1}+0.447827 \theta _{t-1}+0.0306732
   \text{z1}_{t-1}+0.23688 \text{z2}_{t-1}+0.471397 \text{z3}_{t-1}-0.134618 \\
 0.245012 \epsilon_t+0.1296 k_{t-1}+0.232761 \theta _{t-1}+0.0159426
   \text{z1}_{t-1}+0.12312 \text{z2}_{t-1}+0.245012 \text{z3}_{t-1}-0.0699687 \\
 -1.00043 \epsilon _t-1.92563 k_{t-1}-0.950409 \theta _{t-1}-0.23688
   \text{z1}_{t-1}-1.82935 \text{z2}_{t-1}-1.00043 \text{z3}_{t-1}+4.08954 \\
 0.95 \epsilon _t+0.9025 \theta _{t-1}+0.95 \text{z3}_{t-1}+0.0975 \\
   \end{array}
   \right)
\end{gather}}

Substituting  these expressions into equation \refeq{rbcSys} produces
a deterministic system such that, given specific values for 
$(x_{t-1},\epsilon_{t})=(c_{t-1}, k_{t-1},\rcpC_{t-1}, \theta_{t-1}, \epsilon_t)$, we can solve for $z_{1t}(x_{t-1},\epsilon_{t})$, $z_{2t}(x_{t-1},\epsilon_{t})$, $z_{2t}(x_{t-1},\epsilon_{t})$, and $z_{4t}(x_{t-1},\epsilon_{t})$  completely determining
$c_{t}(x_{t-1},\epsilon_{t})$, $k_{t}(x_{t-1},\epsilon_{t})$, $\rcpC_{t}(x_{t-1},\epsilon_{t})$  and $\theta_{t}(x_{t-1},\epsilon_{t})$.\footnote{In this example, the lagged value,  $c_{t-1}$ does not appear in the equation system and consequently plays no role in determining the solution.}  In effect we have 
computed a solution for a model whose trajectories satisfy equation system \refeq{rbcSys}
for one time period and satisfy equation \refeq{rbcLinSys} subsequently.

We are now in a position to compute
$\mathcal{H}^{PF}[g^{0}(x,\epsilon_{t-k+1})]$ or
$\mathcal{H}^{RE}[g^{0}(x,\epsilon_{t-k+1})]$.
Using these functions we can then compute a solution for a model whose trajectories satisfy equation system \refeq{rbcSys}
for two time periods and satisfy equation \refeq{rbcLinSys} subsequently.
\begin{gather}
  \label{eq:1}
  x_t=B x_{t-1} + \phi z_0(x_{t-1},\epsilon) + F \phi \mathcal{Z}^1(x_t)\\
  x_{t+1}=B x_{t} + \phi \mathcal{Z}^1(x_t)
\end{gather}

We will see that all these can precomputed.

Applying formula \refeq{theSeries} produces:

{\tiny
\begin{gather}
  \begin{bmatrix}
c_t\\k_t\\ \rcpC_t\\\theta_t
  \end{bmatrix}=\\%paperCalcsRBCExample xt00
   \left(
   \begin{array}{c}
 0.359845 \epsilon _t+0.692632 k_{t-1}+0.341853 \theta _{t-1}-0.0442851
   \text{z1}_{t-1}+0.658 \text{z2}_{t-1}+0.359845 \text{z3}_{t-1}-0.111552 \\
 0.187032 \epsilon _t+0.36 k_{t-1}+0.17768 \theta _{t-1}+0.0442851
   \text{z1}_{t-1}+0.342 \text{z2}_{t-1}+0.187032 \text{z3}_{t-1}-0.0579799 \\
 -5.34898 k_{t-1}+0.342 \text{z1}_{t-1}-5.08153
   \text{z2}_{t-1}+\text{z4}_{t-1}+3.7794 \\
 \epsilon _t+0.95 \theta _{t-1}+\text{z3}_{t-1}+0.05 \\
   \end{array}
   \right)
\end{gather}
}

and 


{\tiny
%xt01=Private`computeNextXt[{Private`bmat,Private`phimat,Private`fmat,Private`psieps,Private`psic,Private`psiz},solnFunc00PF[[3+Range[3]]],{{cc},{kk},{tt}},{1}]//N//Expand//Simplify
\begin{gather}
  \begin{bmatrix}
c_{t+1}\\k_{t+1}\\ \rcpC_{t+1}\\\theta_{t+1}
  \end{bmatrix}=\\%paperCalcsRBCExample xt00
  \left(
   \begin{array}{c}
 0.471397 \epsilon _t+0.249347 k_{t-1}+0.447827 \theta _{t-1}+0.0306732
   \text{z1}_{t-1}+0.23688 \text{z2}_{t-1}+0.471397 \text{z3}_{t-1}-0.134618 \\
 0.245012 \epsilon_t+0.1296 k_{t-1}+0.232761 \theta _{t-1}+0.0159426
   \text{z1}_{t-1}+0.12312 \text{z2}_{t-1}+0.245012 \text{z3}_{t-1}-0.0699687 \\
 -1.00043 \epsilon _t-1.92563 k_{t-1}-0.950409 \theta _{t-1}-0.23688
   \text{z1}_{t-1}-1.82935 \text{z2}_{t-1}-1.00043 \text{z3}_{t-1}+4.08954 \\
 0.95 \epsilon _t+0.9025 \theta _{t-1}+0.95 \text{z3}_{t-1}+0.0975 \\
   \end{array}
   \right)
\end{gather}}




\subsubsection{Relation to Impulse Response Functions and Stochastic Simulations}
\label{sec:relImp}


Use (Impulse Response Functions?)

Proceeding as though no uncertainty in the parameters.
\href{https://books.google.co.uk/books?id=kLiRDgAAQBAJ&pg=PA77&lpg=PA77&dq=nonlinear+models+impulse+response+function+conditional+expectations&source=bl&ots=1pCjlcgxSu&sig=WA9INrvXVcFhVOd_2wq8-V6alw8&hl=en&sa=X&ved=0ahUKEwid1Y3fvuPWAhUlKsAKHT7lCDkQ6AEITjAF#v=onepage&q=nonlinear%20models%20impulse%20response%20function%20conditional%20expectations&f=false}{book on nonlinear impulse response}
  Potter 1995 2000 and Koop 1996

  from  Elements of Nonlinear Time Series Analysis and Forecasting  Jan G. De Gooijer

CONDITIONAL EXPECTATIONS ASSOCIATED WITH STOCHASTIC PROCESSES
R . A. BROOKS

Law of iterated expectations
\href{http://www.pnas.org/content/103/11/3968.full}{discrete dynamical conditional expectation} 
\label{sec:simple-rbc-model-2}

\begin{itemize}
\item Figure needs attention  \ref{rbcTrunc}
\item Truncation Error
\item path error use solutions for ``wrong'' parameters
\item show formula works
\item later will need to make law iterated expectations applicable
\item use truncation error to show formula works
\end{itemize}


 \begin{gather}
   \xWarg \in{R^L}\,\text{ with }\,\infNorm{\xWarg}  \le \bar{\mathcal{X}}\,\,\forall t> 0 \label{fFamily}.
 \end{gather}

 \begin{itemize}
 \item time invariance ``connects to the dots''  
 \item one step gives information for entire path
 \end{itemize}


\section{Computing Model Solution Error Bounds}
\label{sec:solnerrorbounds}

We will consider time invariant maps that arise form solving a time t optimization problem codified in a collection of systems of equations.  We will assume that we have a countable collection of equations systems that are mutually exclusive and exhaustive.  Given $\epsilon_t, x_{t-1}$  This system of equations produces a solution $x_t$.  The $x_t$ satisfies one and only one of the collection of equations and$x_t$ is (locally) the unique solution for the collection of
equations.  Examples of such systems include typical DSGE models that have one
system of equations, occasionally binding constraints with solutions demonstrating complementary slackness, regime switching models with systems corresponding to the status in each given regime or combinations of the above.

Consequently, will will have an equation system an a gatekeeper logical expression indicating which equation system is in force for a given solution for a give $x_{t-1}, \epsilon_t$

\subsection{Truncation Error}
\label{worst}

\subsection{Path Error}
\label{sec:path-error}
\label{worst}



\paragraph{Quantifying the Inaccuracy of the Proposed Solution}




 Given a proposed solution $x^p_t=g^p(x_{t-1},\epsilon_t)$ define
$G^p(x)\equiv\expct{g^p(x,\epsilon)}$  so that 
  \begin{gather}
E_tx_{t+1}=G^p(g^p(x_{t-1},\epsilon_t))\\
\mathbf{e}_t(x_{t-1},\epsilon)\equiv
\eqnFunc(x_{t-1},x^p_t,E_tx^p_{t+1},\epsilon_t)\\\intertext{Using $G^p$ and $\linMod$ construct the family of trajectories and corresponding $z^p_t(x_{t-1},\epsilon)$ }
   x^p_t(x_{t-1},\epsilon_t) \in{R^L}\,\,\infNorm{x^p_t(x_{t-1},\epsilon_t)}  \le \bar{\mathcal{X}}\,\,\forall t\ > 0
  \end{gather}
   \begin{align}
   z^p_{t}(x_{t-1},\epsilon_t) \equiv& H_{-1}  x^p_{t-1}(x_{t-1},\epsilon_t) + \nonumber\\
 & H_0  x^p_{t}(x_{t-1},\epsilon_t) +  \label{defZ} \\
 & H_1  x^p_{t+1}(x_{t-1},\epsilon_t). \nonumber
   \end{align}








 The proposed solution has a representation given by 
  \begin{gather}
    \label{eq:4}
	 x^p_{t}(x_{t-1},\epsilon) =B x_{t-1}+ \phi \psi_\epsilon\epsilon + (I - F)^{-1} \phi \psi_c +\\ \sum_{\sForSum=0}^\infty F^s \phi z^p_{t+\sForSum}(x_{t-1},\epsilon) 
 \intertext{and}
 	 \expct{x^p_{t+1}(x_{t-1},\epsilon)} =B x^p_{t+k} + \sum_{\sForSum =0}^\infty F^\sForSum \phi z^p_{t+1+\sForSum}(x_{t-1},\epsilon) + (I - F)^{-1} \phi \psi_c \intertext{with}
\mathbf{e}_t(x_{t-1},\epsilon)\equiv
\eqnFunc(x_{t-1},x^p_t,E_tx^p_{t+1},\epsilon_t)
  \end{gather}









  \begin{gather}
    \label{eq:3}
\max_{\{x_{-},\epsilon\}} \infNorm{ \phi \eqnFunc(x_{-},g^p(x_{-},\epsilon),G^p(g^p(x_{-},\epsilon)),\epsilon) }\\
\max_{\{x_{-},\epsilon\}} \infNorm{ \phi \expct{ \eqnFunc(x_{-},g^p(x_{-},\epsilon),G^p(g^p(x_{-},\epsilon)),\epsilon)} }\\
\hat{G}^p(x,\epsilon)=G^p(x,\epsilon)+ \mathcal{I}_e(x,\epsilon)\\
\max_{\{x_{-},\epsilon\}} \infNorm{ \phi \expct{ \eqnFunc(x_{-},g^p(x_{-},\epsilon),\hat{G}^p(g^p(x_{-},\epsilon)),\epsilon)} }\\
\max_{\{x_{-},\epsilon\}} \infNorm{ \phi \expct{ \eqnFunc(x_{-},g^p(x_{-},\epsilon),B g^p(x_{-},\epsilon)+(I-F)^{-1}\psi_c,\epsilon)} }
  \end{gather}






  \begin{gather}
    \label{eq:3}
	 x^\star_{t}(x_{t-1},\epsilon) -	 x^p_{t}(x_{t-1},\epsilon) =
\sum_{\sForSum=0}^\infty F^s \phi (z^\star_{t+\sForSum}(x_{t-1},\epsilon)-z^p_{t+\sForSum}(x_{t-1},\epsilon))     \\
	 x^\star_{t}(x_{t-1},\epsilon) -	 x^p_{t}(x_{t-1},\epsilon) =
\sum_{\sForSum=0}^\infty F^s \phi \mathbf{z_e}_{t+\sForSum}(x_{t-1},\epsilon_t)   \\ 
	\infNorm{ x^\star_{t}(x_{t-1},\epsilon) -	 x^p_{t}(x_{t-1},\epsilon)} \le
\sum_{\sForSum=0}^\infty F^s \phi \infNorm{\mathbf{z_e}_{t+\sForSum}(x_{t-1},\epsilon_t)}    
  \end{gather}


\paragraph{Worst Path: Pessimistic Error Bound}


\subsection{Practical Considerations for Applying the Formula}
\label{sec:practicalformula}


\paragraph{MSNTO}
\paragraph{Assessing a Proposed Solution}
\paragraph{RBC Example for Known Solution}


\begin{figure}
  \centering
\includegraphics{simpBoundsVActual.pdf}  
  \label{rbcTrunc}
  \caption{RBC Truncation Error Bound Versus Actual}
\end{figure}

\paragraph{RBC Example for Unknown Solution}
\paragraph{Algorithm Agnostic: Your Solution Here}




    
\begin{itemize}
\item Choose an Almost  Arbitrary Linear Reference Model
\item Construct Augmented Decision Rule Function (ADR)
\item Along with Augmented Decision Rule Conditional Expectation Function (ADRCE)
\item Compute Bound for Discretization Error
\item Compute Bound for Truncation Error
\end{itemize}










 Given an exact solution $x^\star_t=g^\star(x_{t-1},\epsilon_t)$ define
  \begin{gather}
G^\star(x)\equiv\expct{g^\star(x,\epsilon)} \intertext{then with}
E_tx^\star_{t+1}=G^\star(g^\star(x_{t-1},\epsilon_t))\\
    \label{eq:2}
\eqnFunc(x^\star_{t-1},x^\star_t,E_tx^\star_{t+1},\epsilon_t)=0  \,\, \forall  (x_{t-1},\epsilon_t)\\ \intertext{Using $G^\star$ and $\linMod$ construct the family of trajectories and corresponding $z^\star_t(x_{t-1},\epsilon)$ }
   x^\star_t(x_{t-1},\epsilon_t) \in{R^L}\,\,\infNorm{x^\star_t(x_{t-1},\epsilon_t)}  \le \bar{\mathcal{X}}\,\,\forall t\ > 0
  \end{gather}
   \begin{align}
   z^\star_{t}(x_{t-1},\epsilon_t) \equiv& H_{-1}  x^\star_{t-1}(x_{t-1},\epsilon_t) + \nonumber\\
 & H_0  x^\star_{t}(x_{t-1},\epsilon_t) +  \label{defZ} \\
 & H_1  x^\star_{t+1}(x_{t-1},\epsilon_t). \nonumber
   \end{align}




   The exact solution has a representation given by
	 \begin{gather}
	 x^\star_{t}(x_{t-1},\epsilon) =B x_{t-1}+ \phi \psi_\epsilon\epsilon + (I - F)^{-1} \phi \psi_c +\\ \sum_{\sForSum=0}^\infty F^s \phi z^\star_{t+\sForSum}(x_{t-1},\epsilon) \intertext{and}
	 \expct{x^\star_{t+1}(x_{t-1},\epsilon)} =B x^\star_{t+k} + \sum_{\sForSum =0}^\infty F^\sForSum \phi \expct{z^\star_{t+1+\sForSum}(x_{t-1},\epsilon)} + (I - F)^{-1} \phi \psi_c 
 \intertext{with}
 \eqnFunc(x_{t-1},x^\star_t,E_tx^\star_{t+1},\epsilon_t)=0  \,\, \forall  (x_{t-1},\epsilon_t)\\ 
	 \end{gather}



  \begin{itemize}
  \item Law of iterated expectations applies for decision rules.
  \item show why system approach important
  \item Exact solution have zero discrepancy
  \item Compute deviations  DR approximation error
  \item Expectation computation approximation error
  \item Series truncation error
  \item Don't need to iterate a long path since all the potentials 
errors attainable in first iteration step by searching across initial conditions
\item condition number and error bounds
  \item why system view better
  \item factor out expectation function
  \item allow loss function for model error
  \item integration error 
  \item bound other people's solutions, compare to bound Euler errors
  \item show can determine number series terms then accuracy depends on number of nodes
  \item compute errors and max norm for all relevant state $(x_{t-1},\epsilon_t)$ and consequently for all the conditional expectations
  \item precompute  conditional expectations for basis since amaseries is
just a linearly weighted sum
\item what is best way to quantify characterize impact of error along the path
inaccurately computed future trajectory
\item can discover unknown solutions
\end{itemize}






\section{An Algorithm for Improving Proposed Time Invariant Solutions}
\label{sec:algoforsoln}

\subsection{Algorithm Overview}
\label{sec:algoverview}

\begin{itemize}
\item modify equation systems to allow application of law of iterated expectations
\item choose $\linMod$ of appropriate dimensions
\item choose accuracy goal
\item choose $\tau$ number of periods of $z_t$ terms
\end{itemize}
  \begin{algorithm}[H]
compute \ADRCE\  function\\
construct $\mathbf{G}$ function\\
solve system to get $x_t,z_t$ at interpolation points for $x_{t-1},\epsilon_t$\\
assess accuracy\\
repeat till accuracy goal achieved
\caption{Improve Approximate \ADR }
  \end{algorithm}


This series representation for deterministic maps,
broadly applicable for nonlinear rational expectations models 
leads to a formulas for accuracy bounds for any proposed solution.
As an important component in an algorithm for
constructing approximate solutions it
facilitates exploiting recursive computation of the solutions for complicated models.
The algorithm has been implemented in
Mathematica code that can compute solutions for
nonlinear models with occasionally binding constraints and/or regime switching models.






\subsubsection{Generality}
\label{sec:generality}




\begin{itemize}
\item Time Invariant Maps
\item Bounded Solutions
\item Exists a Representation Problem is to Find One
\item Algorithm terminates with a Proposed Solution With Solution
  No further away from a true solution than the specified tolerance 
\item No Existence Guarantees or Uniqueness Guarantees.
\item Realizable
\end{itemize}


Consider a family of {\it time invariant } stochastic functions:
The $x_{-1}$ is an  $L$ dimensional state vector and $\epsilon$ is a $K$ dimensional ``shock'' vector that together index
individual trajectories for future state vectors.  
Each member of the family characterizes the evolution of a {\em deterministic} trajectory of values.\footnote{Subsequent sections describe how these deterministic trajectories are useful for representing a wide array stochastic model solutions.}

It will prove useful to also define a time invariant deterministic function $\XtFuncTI\equiv \expctEps{\xtFuncTI}$ and denote
\begin{gather*}
\xsubtFunc{t+k}{(x_{t-1},\epsilon_t)}\equiv\begin{cases}
\xtFunc{(x_{t-1},\epsilon_t)} &k=0\\
\XtFunc{(\xsubtFunc{t+k-1}{(x_{t-1},\epsilon_t)})} &k>0
\end{cases}
\end{gather*}


Iterating conditional expectations of stochastic functions forward will lead
us to a series representation useful for representing dynamic model solutions.
Many rational expectations models have solutions characterized by a stochastic function $\mathcal{X}(x_{t-1},\epsilon_t):\mathcal{R}^n \times \mathcal{R}^k \rightarrow \mathcal{R}^n$ $\epsilon_t \sim iid$.
Consider Iterating the function $\mathcal{X}$ forward by 
recursively applying $\XtFunc{}$ to compute a solution path
\begin{gather}
\underbrace{(x_{t-1},\epsilon_t)} 
\underbrace{{\mathcal{X}}(x_{t-1},\epsilon_t)}
\underbrace{\xsubtFunc{t+1}{(x_{t-1},\epsilon_t)}}
\underbrace{\xsubtFunc{t+2}{(x_{t-1},\epsilon_t)}}
\underbrace{\ldots}
\intertext{Now, suppose this iteration produces bounded trajectories }
\infNorm{\xsubtFunc{t+k}{(x_{t-1},\epsilon_t)}}  \le \bar{\mathcal{X}}\,\,\forall s\ge 0 
 \end{gather}
then it will then, be possible to write down a useful 
series representation for
the function $\mathcal{X}(x_{t-1},\epsilon_t)$.
These series will be based upon
discrepancies from an ``arbitrary'' Blanchard-Kahn linear dynamic
system.\footnote{As seen below, the linearity of the saddle point model
  will not preclude applying the technique to highly non linear models.}




  {Begin by Solving a Deterministic System at time $t$}
{\small

  \begin{itemize}
  \item For any given $\left (  \begin{bmatrix}
c_{t-1}\\k_{t-1}\\ \rcpC_{t-1}\\\theta_{t-1}
  \end{bmatrix}, \epsilon_t \right )=\tArg$ 
compute
  \begin{gather}
    \label{eq:3}
    x_t^1\tArg=B x_{t-1} + \phi \psi_e\epsilon_t + \phi z^1_t\tArg\\
    E_t(x^1_{t+1}\tArg)=B x^1_{t}\tArg
  \end{gather}
\item The model equations provide a deterministic system  for computing $  z^1_t=\begin{bmatrix}
    z^1_{1t}\tArg\\
    z^1_{2t}\tArg\\
    z^1_{3t}\tArg\\
    z^1_{4t}\tArg
  \end{bmatrix}$.
\item The solution satisfies the nonlinear model equations for one 
period and the stand-in linear model for subsequent periods.
\item Assess accuracy
  \end{itemize}
}




%   {Assess One Period Solution Accuracy}
%   \begin{itemize}
%   \item Show graphs of solution
%   \item Show graphs of error
%   \item Report accuracy assessment bounds
%   \end{itemize}



  {Nonlinear 2 Periods: Solve time $t$ Deterministic
    System }
{\small
  \begin{itemize}
  \item Compute $Z^1(x)= E_t(z^1_t(x,\epsilon_t))$
  \item For any given $\tArg$ 
compute
{\small
  \begin{gather}
    x_t^2\tArg=B x_{t-1} + \phi \psi_e\epsilon_t + \phi z^2_t\tArg + F \phi Z^1(x^2_t) \label{bothS}\\
    E_t(x^2_{t+1}\tArg)=B x^2_{t}\tArg+ \phi Z^1(x^2_t)
  \end{gather}
}
\item The model equations provide a deterministic system  for computing $  z^2_t=\begin{bmatrix}
    z^2_{1t}\tArg\\
    z^2_{2t}\tArg\\
    z^2_{3t}\tArg\\
    z^2_{4t}\tArg
  \end{bmatrix}$.
\item The solution satisfies the nonlinear model equations for two 
period and the stand-in linear model for subsequent periods.
\item unlike the first step, $x^2_t$ appears on both sides of equation \refeq{bothS}
\item  surprisingly, a simple fixed point iteration solves the nonlinear system
\item Assess accuracy
  \end{itemize}
}

\subsubsection{Algorithm Pseudo-code}
\label{sec:pseudocode}

\begin{enumerate}
\item Specify a (collection) model equations systems(s)
\item specify a linear reference model
\item specify an initial guess for decision rule $\xWargK$ obtain conditional expectations function
\item decide upon number of Conditional Expectations function approximation.  Shorter means more truncation and higher error longer means including more terms from series In effect imposes nonlinear constraints for number of period specified and the linear reference model thereafter
\item algorithm uses conditional expectation and model equations to produce an improved approximation to the decision rule.
\item the algorithm finds a solution to the system that constrains $x_t= x_g$ determining $x_t,z_t$ at the interpolation points for $x_{t-1},\epsilon_t$  This can be done in parallel
\item the data from the interpolation points is used to produce an updated decision rule
\end{enumerate}


\subsection{Function Approximation Representation}
\label{sec:funcApproxRep}

\subsubsection{General Issues}
\label{sec:generalissues}

\paragraph{Function composition}



\paragraph{Correct Expectations}

\paragraph{Precomputing Integrals}


\subsubsection{Smolyak Interpolation}
\label{sec:smolyakinterp}

\paragraph{An-isotropic}

\paragraph{Ergodic Set}
\paragraph{MSNTO}

\subsection{RBC Example}
\label{sec:rbc-example}


\subsubsection{Approximating the Known Solution: $U(c) = Log(c)$ }
\label{sec:recov-known-solut}

\subsubsection{Approximating an Unknown Solution: $U(c) \ne Log(c)$ }
\label{sec:unknown-solutions}
\subsection{ Other Examples}
\label{sec:otherexamples}

\subsubsection{Bernstein Model}
\label{sec:bernstein-model}



\label{sec:regime-switch-model}

\subsubsection{Occasionally Binding Constraints}
\label{sec:obc-solut}

The algorithm we have described,
uses a proposed deterministic map
characterizing the evolution of expected values for
the dynamic system going forward. It then solves
a deterministic problem at time t to improve the proposed solution.
There is nothing in the algorithm that precludes accommodating  inequality
constraints.\footnote{See section \ref{sec:regime-switch-model} characterizing
  models with regime switching.}

\begin{gather}
  h_i(x_{t-1},x_{t},x_{t+1},\epsilon_t)=h^{det}_{io}(x_{t-1},x_{t},\epsilon_t)+\sum_{j=1}^{p_i} [h^{det}_{ij}(x_{t-1},x_{t},\epsilon_t)h^{nondet}_{ij}(x_{t+1})]=0
\end{gather}

  {Assessing Rational Expectations Solution Accuracy}

The $m$ function codifies changes in model equations due to
inequality constraints or regime switching.$(\varpi \in \{1,\ldots,n\})$
we are  interested in finding a time invariant function $g^\ast$ that satisfies
 \begin{gather}
   \begin{split}
 h_{\varpi}(x_{t+s-1},g^\ast(x_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(x_{t+s-1},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) \label{theProblem} \\
\varpi= m(x_{t+s-1},g^\ast(x_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(x_{t+s-},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) 
   \end{split}\intertext{ for all $s>0$ where $\mathcal{H}$ is an operator,    that maps stochastic to deterministic functions}
  \end{gather}


For example, the Euler equations for the  neoclassical growth  model with
irreversible investment
\label{sec:simple-rbc-model-ext} can be written as
\begin{gather}
h_{10^{det}}(\cdot)=\frac{1}{c_t},\,\,
h_{11}^{det}()=\alpha \delta k_{t}^{\alpha-1} ,\,\,
h_{11}^{nondet}(\cdot)=E_t \left (\frac{\theta_{t+1}}{c_{t+1}} \right )\\
h_{20}^{det}(\cdot)=c_t + k_t-\theta_tk_{t-1}^\alpha,\,\,
h_{21}^{det}(\cdot)=0\\
h_{30}^{det}(\cdot)=\ln \theta_t -(\rho \ln \theta_{t-1} + \epsilon_t),\,\,
h_{31}^{det}(\cdot)=0\\
(k_t>0\land\theta_t>0\land c_t>0) \land
( (I_t>\gamma I^\ast \land \lambda_t>0)\lor (I_t=\gamma I^\ast \land \lambda_t\ge0))
\end{gather}
Since we will be working with models where expectations are computed at time t, $\epsilon_t$ is known.  Once again, the only stochastic components are those with time subscripts greater than t.

\subsubsection{regime switching}
\label{sec:regime}

\paragraph{Proposed Conditional Expectation}

\paragraph{Deterministic Problem}

\paragraph{Conditional Expectation Update}


\subsubsection{RBC Example}
\label{sec:generalRBCExample}

\paragraph{unknown solutions}
\begin{description}
\item[Solution]
\item[Error Bound]
\end{description}

\paragraph{unknown solutions occasionally binding constraints}
\begin{description}
\item[Solution]
\item[Error Bound]
\item[resources]\
  \begin{itemize}
  \item 
  \end{itemize}
\end{description}



For example,
consider  the Barthelemy and Marx  Model 2: Regime Switching\cite{marxbarthelemy2012}


\cite{troy2007}
\begin{gather}
  \label{eq:4}
  i_t =E_t \pi_{t+1} + r_t\\
r_t= \rho r_{t-1} +u_t\\
i_t=\alpha_{s_t} \pi_t
\end{gather}

Bounded solutions if and only if all eigenvalues of 
\begin{gather}
  \label{eq:5}
  \begin{bmatrix}
    \frac{1}{|a_1|}&0\\
0&    \frac{1}{|a_2|}
  \end{bmatrix}
  \begin{bmatrix}
    p_{11}&p_{12}\\p_{21}&p_{22}
  \end{bmatrix}
\end{gather}
 are inside unit circle




  {Assessing Accuracy}
{\small

  \begin{itemize}
  \item As with series approximation,
 construct a family of bounded trajectories and compute
$  z_{t+s}(x_{t-1},\epsilon_t)$ as  %\footnote{These $z$ functions will soon prove useful in an algorithm for computing unknown trajectories like \refeq{fFamily}.}:
{
\begin{gather}
  z_{t+s}(x_{t-1},\epsilon_t) \equiv\\
   \begin{split}
 h_{\varpi}(\mathcal{X}_{t+s-1},g^\ast(\mathcal{X}_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(\mathcal{X}_{t+s-1},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) \label{theProblem} \\
\varpi= m(\mathcal{X}_{t+s-1},g^\ast(\mathcal{X}_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(\mathcal{X}_{t+s-},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) 
   \end{split}
  \end{gather}
}
\item The formula \refeq{theSeries} provides information about how much $x_{t}$ would need
to change in order for the trajectory to honor the constraints along the path
\item An exact solution should produce zero for all the $z$ functions
\item One can use a truncated series to carry out the approximation for changes in $x_t$
\item Useful to have measures of accuracy that don't rely upon knowing the solution beforehand
\item can adopt Judd approach for loss function for characterizing importance of errors
  \end{itemize}
}


  {Barthelemy and Marx  Model 2: Regime Switching
\cite{marxbarthelemy2012}}


\cite{troy2007}
\begin{gather}
  \label{eq:4}
  i_t =E_t \pi_{t+1} + r_t\\
r_t= \rho r_{t-1} +u_t\\
i_t=\alpha_{s_t} \pi_t
\end{gather}

Bounded solutions if and only if all eigenvalues of 
\begin{gather}
  \label{eq:5}
  \begin{bmatrix}
    \frac{1}{|a_1|}&0\\
0&    \frac{1}{|a_2|}
  \end{bmatrix}
  \begin{bmatrix}
    p_{11}&p_{12}\\p_{21}&p_{22}
  \end{bmatrix}
\end{gather}
 are inside unit circle



 \begin{description}
 \item[model equation function] \
   \begin{gather}
     f(x_{t-1},x_t,E_t(x_{t+1}|\iSet),\epsilon_t)=0 \intertext{later, mutually exclusive exhaustive}
     \{f_1(x_{t-1},x_t,E_t(x_{t+1}|\iSet),\epsilon_t)=0,b_i(x_{t-1},x_t,E_t(x_{t+1}|\iSet),\epsilon_t),\\
     f_2(x_{t-1},x_t,E_t(x_{t+1}|\iSet),\epsilon_t)=0,b_i(x_{t-1},x_t,E_t(x_{t+1}|\iSet),\epsilon_t),\\ \vdots,\\ f_K(x_{t-1},x_t,E_t(x_{t+1}|\iSet),\epsilon_t)=0,b_i(x_{t-1},x_t,E_t(x_{t+1}|\iSet),\epsilon_t)\}
   \end{gather}
 \item[genxkzk]
    \begin{gather}
      \begin{bmatrix}
        x_{t-1}\\x_t\\E_t(x_{t+1}|\iSet)\\\epsilon_t
      \end{bmatrix}= \mathcal{G}(x_{t-1},\epsilon_t|\XtFunc(\cdot),x_t^g)
    \end{gather}
  \item[Augmented Decision Rule (\ADR)]
    \begin{gather}
      \begin{bmatrix}
        x_t\\z_t
      \end{bmatrix}=
      xtFunc(x_{t-1},z_t)
    \end{gather}
  \item[Augmented Decision Rule Conditional Expectation (\ADRCE)] 
     \begin{gather}
      E_t\left ( \begin{bmatrix}
         x_t\\z_t
       \end{bmatrix} \right )=
       XtFunc(x_{t-1},z_t)
     \end{gather}
\end{description}

\begin{itemize}
\item backward looking equations no need to solve
\end{itemize}
\section{Future Work}
\label{sec:future}

\begin{itemize}
\item FRBUS VAR expectations and Agent Based Models:
\item confidence intervals other  summary statistics how often constrained, how long
\item replace stochastic sims
\item time varying:  regime switching, hammers need nails
\item The algorithm for discovering unknown solutions does not rely on time invariance.  Setup for iterated expectations
\item SVMR and other ML universal approximators
  \begin{itemize}
  \item less need for an-isotropic
  \item less need to find ergodic set
  \end{itemize}
Support vector machines occupy a prominent place applied machine learning
Additionally they can reduce the burden of computation as they determine 
a subset of points that are important in characterizing a given function.
Kernel trick,  powerful representation quadratic programming solution to compute there are on-line techniques for adding and deleting individual points from the representation.  A weighted sum of kernel functions RBF, wavelet, special forms for time series.
  This paper proposes a new series representation for bounded solutions to dynamic models. This series representation can be used
to determine a series representation for time invariant discrete time
maps that characterize the solutions to many models. Consequently,
the technique constitutes an important component in a technique for
accurately characterizing the solutions for a wide array of nonlinear
rational expectations models. It can also provide a formula for computing accuracy bounds for any proposed time invariant model solution. The series representation serves as an important component
in an algorithm for constructing approximate solutions for nonlinear
rational expectations models.
The technique recursively computes solutions that honor the constraints for successively longer horizons.  The solutions computed by
the technique accommodate the possibility that model trajectories can
depart from and re-engage inequality constraints as well as transition
between various regimes.
This paper applies support vector machine function approximation to
reduce the computational burden associated with representing the
unknown, potentially highly nonlinear stochastic functions that arise in
solving dynamic models with occasionally binding constraints.


Support vector machines 
have become an essential tool in contemporary machine learning research
where computer scientists exploit their flexibility and
computational tractability in modeling complex high dimensional data.
Like many other function approximation approaches,
support vector machines represent functions as a linearly weighted sum
of a family of basis functions.  They differ from other approaches in  the
use of ``hinge loss functions'' that generate
an easy to solve
quadratic programming problem(QPP) for determining the weights.
The solution of this QPP identifies a subset of points, the support vectors,
that are influential in the representation.  With strategically chosen
basis functions, this can dramatically reduce the number of terms needed
to approximate a function to a given level of accuracy.




\item function composition Fa Di Bruno
\item divide and conquer solution space initially collect results stochastic steady state at center of divided region with overlap
\item dynamical system theory implications
\item relation to Koopman Operators
\item Implications of almost arbitrary.  Small F eigenvalues versus small z values
\item implement other languages
\end{itemize}

\section{Conclusions}
\label{sec:conc}




\bibliographystyle{plainnat}
\bibliography{anderson,files}
\appendix
  {Recursive updating}
{\small
  \begin{itemize}
  \item Given a linear reference model,$\linModMats$,  any bounded 
sequence of deterministic maps $\{\detF{T},\ldots,\detF{0}\}, \,\, T>0$ generates 
a series representation
  \begin{gather}
     \label{eq:2}
	 \mathcal{X}_{t}(x_{t-1}) =B x_{t-1}+  (I - F)^{-1} \phi \psi_c +\\ \sum_{\sForSum=0}^{T-1} F^s \phi z_{T-\sForSum}(x_{t-1}) \intertext{it is straightforward to 
update to a new series representation for the map trajectories resulting from prepending an additional deterministic map to the sequence $\detF{T+1},\ldots,\detF{0}\}$}
	 \mathcal{X}_{t}(x_{t-1}) =B x_{t-1}+  (I - F)^{-1} \phi \psi_c +\\ 
\phi z_{T+1}(x_{t-1}) + \sum_{\sForSum=1}^{T} F^s \phi z_{T+1-\sForSum}(x_{t-1}) 
  \end{gather}
\item 
  \end{itemize}
}



%\footnote{We set $\psi_c=0$  without loss of generality 
%in the expressions that follow merely to save on notation.}


{The model of interest and a linear reference model}
\begin{itemize}
\item The target model is of the form
\begin{gather}
\nlEqnLHS{x_{t-1}}{x_t}{x_{t+1}}{t}=\\
\nlEqn{x_{t-1}}{x_t}{x_{t+1}}{t}=0\\ \label{refMod}
 (L \times 1) +  (L \times L ) \cdot (L \times 1)\\
\nlEqnSel{x_{t-1}}{x_t}{x_{t+1}}{t}\\ 
\end{gather}



\item The algorithm constructs a sequence of stochastic 
functions 
  \begin{gather}
	 \stochF{k}\initXE =B x_{t-1}+ (I - F)^{-1} \phi \psi_c+  \phi \psi_\epsilon\epsilon_0  +\\ \phi z^{\{k\}}\initXE+ \sum_{\sForSum=1}^{k-1} F^s \phi z^{\{k-\nu\}}_{\sForSum}\initXE 
\label{theSeries}
  \end{gather}

\item One can show that the algorithm begins with solutions satisfying 
the linear reference model $\linModMats$ and
constructs a sequence of functions
 honoring the target model equations for successively 
longer solution horizons.
  \end{itemize}

\section{more from above}
\label{sec:more-from-above}


{Early iterations}
{\small
  \begin{itemize}
  \item $k=0$: Completely ignore the target model
  \begin{gather}
z^{\itSup{0}}\initXE=0 \intertext{ so that}
\stochF{0}\initXE=B x_{-1} + (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0  \intertext{initialize the sequence of deterministic maps with }
\{\detF{0}\initX=B x \}
  \end{gather}
  \item $k=1$: Employ the target model for one period. Use
  \begin{gather}
x_0\initXE=B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + \phi z^{\itSup{1}}\initXE\\
E(x_1\initXE)=\detF{0}(x_0\initXE) \intertext{in the deterministic equation 
\refeq{refMod} to determine $z^{\itSup{1}}\initXE$ so}
\stochF{1}\initXE = B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + 
\phi z^{\itSup{1}}\initXE \intertext{%Define $Z^{\itSup{1}}(x)\equiv E \left [ z^{\itSup{1}}\initXEN | x \right ]$ and 
 augment the sequence}
\{\detF{1}\initX=B x  + \phi Z^{\itSup{1}}(x),\detF{0}\}
  \end{gather}
  \end{itemize}
}



{Early iterations}
{\small
  \begin{itemize}
  \item $k=2$: Employ the target model for two periods. Use
  \begin{gather}
x_0\initXE=B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 +\\ \phi z^{\itSup{2}}\initXE + F \phi Z^{\itSup{1}}(x_0\initXE)\\
E(x_1\initXE)=\detF{1}(x_0\initXE) \intertext{in the deterministic equation 
\refeq{refMod} to determine $z^{\itSup{2}}\initXE$}
\stochF{2}\initXE = B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 +\\ \phi z^{\itSup{2}}\initXE + F \phi Z^{\itSup{1}}(x_0\initXE)\intertext{
Define $Z^{\itSup{2}}(x)\equiv 
E \left [ z^{\itSup{2}}\initXEN | x \right ]$}
\{\detF{2}\initX=B x  + \phi  Z^{\itSup{2}}(x) + F \phi Z^{\itSup{1}}(x_0\initXE),
\detF{1}\initX,\detF{0}\initX\}
  \end{gather}
  \end{itemize}
}




{Early iterations}
{\small
  \begin{itemize}
  \item $k=3$: Employ the target model for two periods. Use
  \begin{gather}
x_0\initXE=B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + \phi z^{\itSup{3}}\initXE +\\ F \phi Z^{\itSup{2}}(x_0\initXE) + F^2 \phi Z^{\itSup{1}}(\detF{2}(x_0\initXE))\\
E(x_1\initXE)=\detF{2}(x_0\initXE) \intertext{in the deterministic equation 
\refeq{refMod} to determine $z^{\itSup{3}}\initXE$}
\stochF{3}\initXE = B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + \phi z^{\itSup{3}}\initXE + \\F \phi Z^{\itSup{2}}(x_0\initXE) +F^2 \phi Z^{\itSup{1}}(\detF{2}(x_0\initXE))\intertext{
Define $Z^{\itSup{3}}(x)\equiv 
E \left [ z^{\itSup{3}}\initXEN | x \right ]$}
\detF{2}\initX=B x  + \phi  Z^{\itSup{3}}(x) + 
F \phi Z^{\itSup{2}}(x_0\initXE) + \\F^2 \phi Z^{\itSup{1}}(\detF{1}(x_0\initXE))
  \end{gather}
  \end{itemize}
}



{General iterations}
{\small
  \begin{itemize}
  \item $k=K+1$: Employ the target model for $K+1$ periods. Use
  \begin{gather}
x_0\initXE=B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + \phi z^{\itSup{K+1}}\initXE +\\ \sum_\sForSum^K F^\nu \phi Z^{\itSup{K+1-s}}\detComp{x_0\initXE} \\
E(x_1\initXE)=\detF{K}(x_0\initXE) \intertext{in the deterministic equation 
\refeq{refMod} to determine $z^{\itSup{K+1}}\initXE$}
\stochF{K+1}\initXE = B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + \phi z^{\itSup{K+1}}\initXE + \\
\sum_\sForSum^K F^\nu \phi Z^{\itSup{K+1-s}}\detComp{x_0\initXE} 
  \end{gather}
  \end{itemize}
}

\section{Still another}
\label{sec:still-another}

We are now in a position to easily compute 
 \begin{gather}
\xIter{0}{x}=\mathcal{H}^{PF}[g^{0}(x,\epsilon_{t-k+1})]=
\mathcal{H}^{RE}[g^{0}(x,\epsilon_{t-k+1})]= 
B x+  (I - F)^{-1} \phi \psi_c
 \end{gather}
\begin{gather}
\underbrace{x_{t-1},0} 
\underbrace{\xNow{0},\zNow{0}}
\underbrace{\xNowtp{0}=\xIter{0}{x_{t}},0}
\end{gather}
In other words, from time t onwards, this approximation assumes 
that our stand-in
 convergent linear model completely characterizes the solution.\footnote{
The algorithm terminates with an approximation for 
the time invariant function $g^\ast$.}  It is unlikely that $g^0(x_{t-1},\epsilon_{t})$
satisfies equation \refeq{theProblem} for arbitrary $x_{t-1}$ even for $s=0$.
We can get a pessimistic upper bound for how far we are away from the solution 
by computing the infinity norm of substituting the variables from into Equation \refeq{theProblem}.
% \footnote{The values in Table \ref{truncTab} illustrate how
%pessimistic this approximation can be, but it turns out we can expect the accur%acy of  approximate truncation errors improve as we expand the approximation series.}

Next, we compute a solution in which the nonlinear equations are satisfied at time t alone while the stand-in linear system holds sway subsequently.
Our algorithm will  compute
\begin{gather}
\underbrace{x_{t-1},0} 
\underbrace{\xNow{1},\zNow{1}}
\underbrace{\xNowtp{1}=\xIter{1}{x_{t+1}},0}
\underbrace{\xNowtp{1}=\xIter{0}{x_{t+1}},0}
\end{gather}
that satisfy the nonlinear equation system at time t.
For a given $x_{t-1}$, we substitute
\begin{gather}
x_{t-1}, \,\,
  x_t=B x_{t-1} + \phi z^1(x_{t-1},\epsilon_t) , \,\,
  x_{t+1}=B x_{t} + \phi \mathcal{Z}^1(x_t)
\end{gather}
into the non linear equation system to determine the 
$z^1(x_{t-1},\epsilon_t)$ and consequently $x_t^1(x_{t-1},\epsilon_t)$.
We then have
 \begin{gather}
 g^1(x_{t-1},\epsilon_{t})=  
B x_{t-1}+ \phi \psi_\epsilon\epsilon_{t} +
\phi \psi_\epsilon z^1(x_{t-1},\epsilon_t)+
 (I - F)^{-1} \phi \psi_c\\ \label{firstIter}
z^{1}_{t+i}(x_{t-1},\epsilon_{t-1})=0 \,\, \forall i \ge 1
 \end{gather}
Now, $g^1(x_{t-1},\epsilon_{t})$
satisfies equation \refeq{theProblem} for arbitrary $x_{t-1}$ at time $t$ with $s=0$.  However, it probably does not solve the system for $s>0$.
We can get a pessimistic upper bound for how far we are away from the solution 
by substituting the variables and computing the infinity norm for plausible values of $x_{t-1}$.
\begin{gather}
\xNow{1},\xNowtp{1},\xNowtp{1}=\xIter{1}{x_{t+1}}
\end{gather}
into Equation \refeq{theProblem}. If the accuracy of the approximation is adequate, we need not compute any further  terms in the power series.  If not,
we are now in a position to compute our choice of
$\mathcal{Z}^1(x)=\mathcal{H}^{PF}[g^{1}(x,\epsilon_{t-k+1})]$ or
$\mathcal{Z}^1(x)=\mathcal{H}^{RE}[g^{1}(x,\epsilon_{t-k+1})]$.

%  \begin{gather}
%  g^0(x_{t-1},\epsilon_{t})=  
% B x_{t-1}+ \phi \psi_\epsilon\epsilon_{t} +
%  (I - F)^{-1} \phi \psi_c\\ \label{firstIter}
% z^{0}_{t+i}(x_{t-1},\epsilon_{t-1})=0 \,\, \forall i \ge 0
%  \end{gather}
%  \begin{gather}
%  g^0(x_{t-1},\epsilon_{t})=  
% B x_{t-1}+ \phi \psi_\epsilon\epsilon_{t} +
%  (I - F)^{-1} \phi \psi_c\\ \label{firstIter}
% z^{0}_{t+i}(x_{t-1},\epsilon_{t-1})=0 \,\, \forall i \ge 0
%  \end{gather}

Using these functions we can then compute a solution for a model whose trajectories satisfy equation system \refeq{rbcSys}
for two time periods and satisfy equation \refeq{rbcLinSys} subsequently.

\begin{gather}
\underbrace{x_{t-1},0}
\underbrace{\xNow{2},\zNow{2}} 
\underbrace{\xNowtp{2}=\XNow{2}{t}{t},\ZNow{2}{1}{t}}
\underbrace{\XNow{1}{t+1}{t},0}\underbrace{\XNow{0}{t+2}{t+1},0}    \intertext{where we have solved for $\xNow{2}$ and $\zNow{2}$ to satisfy the  equation system \refeq{theProblem}.}
\xNowtp{2}=\XNow{1}{t}{t}
\end{gather}

\begin{gather}
  \label{eq:1}
  x_t=B x_{t-1} + \phi z^2(x_{t-1},\epsilon) + F \phi \mathcal{Z}^1(x_t)\\
  x_{t+1}=B x_{t} + \phi \mathcal{Z}^1(x_t)+
 (I - F)^{-1} \phi \psi_c\\
  x_{t+2}=B x_{t+1} + (I - F)^{-1} \phi \psi_c\\
  x_{t+3}=B x_{t+2} + (I - F)^{-1} \phi \psi_c
\end{gather}

The next iteration highlights an aspect of the algorithmic step which allows us
to continue to exploit the series representation.

{\small
\begin{gather}
\underbrace{x_{t-1},0}
\underbrace{\xNow{3},\zNow{3}} 
\underbrace{\xNowtp{3}=\XNow{3}{t}{t},\ZNow{3}{1}{t}} 
\underbrace{\XNow{2}{t+1}{t},\ZNow{2}{2}{t+1}} 
\underbrace{\XNow{1}{t+3}{t+2},0}
\underbrace{\XNow{0}{t+4}{t+3},0}
\end{gather}
}

\begin{gather}
  \label{eq:1}
  x_t=B x_{t-1} + \phi z^3(x_{t-1},\epsilon) + F \phi \mathcal{Z}^2(x_t)+ F^2 \phi \mathcal{Z}^1(x_t)+
 (I - F)^{-1} \phi \psi_c\\
  x_{t+1}=B x_{t} + \phi \mathcal{Z}^1(x_t)+ F^2 \phi \mathcal{Z}^1(x_t)+
 (I - F)^{-1} \phi \psi_c\\
  x_{t+2}=B x_{t+1} + (I - F)^{-1} \phi \psi_c\\
  x_{t+3}=B x_{t+2} + (I - F)^{-1} \phi \psi_c\\
  x_{t+4}=B x_{t+2} + (I - F)^{-1} \phi \psi_c
\end{gather}



In the general iteration step we will have
\begin{gather}
  \label{eq:1}
  x_t=B x_{t-1}  \phi \psi_\epsilon \epsilon_t + (I - F)^{-1} \phi \psi_c +
\sum_{s=0}^{k} F^s \phi \mathcal{Z}^{k-s}(x_{t+s}^{k+1})\\
  x_{t+s}^{k+1}=\Pi_{s=0}^k \xIter{s}{ x_{t}^{k+1}}
\end{gather}


\section{Distinct Roles}
\label{sec:distinct-roles}



  {Algorithm based on basic properties of deterministic maps.}
  \begin{itemize}
  \item     The $\epsilon$'s  play two distinct roles in the algorithm.
To see the two roles, consider a family of stochastic functions 
$\{\stochF{T},\ldots,\stochF{0}\}$ with 
$\stochF{k}(x,\epsilon_t): {\mathcal{R}}^{L+1} \rightarrow \mathcal{R}^L$ 
    \begin{enumerate}
\item I assume  $\epsilon_t \sim $ iid. Knowing the distribution 
of the $\epsilon_t$ makes it possible to compute 
a corresponding family of deterministic functions $\detF{k}(x)=E( \stochF{k}(x,\epsilon_t)|x) $
    \item I assume that at some initial time, say t=0, a particular 
 $\epsilon_0$ draw 
along with an $x_{-1}$, determines a unique $x_0=\stochF{T}\initXE$.
Subsequently, $x_{t}=\detF{{T-t}}(x_{t-1})\,\, \forall t>0$
    \end{enumerate}
 \item The series representation requires a unique trajectory beginning with $x_0=\stochF{T}\initXE$ and 
evolving forward from any $x_0$ given by, a potentially time varying deterministic map,  $x_t=\detF{T-t}(x_{t-1})\,\, \forall t>0$. 
  \end{itemize}
    













\end{document}

