\documentclass[12pt]{article}


% \usepackage[authoryear]{natbib}
% \usepackage{amsmath}
% \usepackage{hyperref}
% \usepackage{hyperref}
% \usepackage{geometry}
% \usepackage{graphicx}
% \usepackage{amsfonts}

\input{../../paperProduction/occBind/docs/AMArepresentationNewCmds}



\author{Gary S. Anderson\thanks{The analysis and conclusions set forth are those of the author and do not indicate concurrence by other members of the research staff or the Board of Governors. I would like to thank Luca Guerrieri, Christopher Gust, Hess Chung, Benjamin Johannsen  and Robert Tetlow for their comments and suggestions.  Special thanks to Luca Guerieri for first noticing that the series representation formulation could lead to an error bound for model solutions.}}

\title{A New Series Representation for Time Invariant Maps that
 Arise in  Nonlinear Dynamic Stochastic Economic Models}


\begin{document}

\maketitle

\begin{abstract}
This paper proposes a new series representation for dynamic model solution paths and uses this representation to construct error bounds for proposed solutions to a very general class of dynamic stochastic models including models subject to occasionally binding constraints and/or regime switching.
This which, incorporating the impact of errors across  all the equations in 
the system makes it possible to compute
apriori estimates of the computational cost for a
given level of accuracy of each component of the solution.
These error bounds should prove useful for assessing the accuracy of any
proposed model solution regardless of the source.


In addition, the paper also proposes an new algorithm for solving these types of models.
The series representation makes it possible to reliably improve upon an initial
guess for a stochastic model solution by solving a 
potentially complicated, but deterministic problem in the initial time period.
Like the error bound formula, the algorithm is appropriate for models with occasionally binding constraints and/or regime switching. 


The  paper uses a particular implementation of the algorithm to
demonstrate how to use the 
series representation in conjunction with 
Smolyak polynomial function approximation to avoid numerical integration
and to exploit the high degree of parallelism available in the algorithm.







\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction and Summary}





Stochastic dynamic non linear economic
models increasingly embody  occasionally binding constraints (OBC) essential
Since \cite{Christiano2000} a host of
authors have described a variety of approaches. 
\cite{holden15:_exist_dsge,guerrieri15:_occbin,benigno09,hintermaier10,brumm10,nakov08,haefke98,nakata12,gordon11,billi11,Hintermaier2010,Guerrieri2015}
This paper provides yet another.  This new series representation provides  a coherent framework for attacking a wide variety of complicated nonlinear models.
The framework provides a new way to bound the error one can expect from
employing a given proposed model solution and leads to a
algorithm with  components similar to parameterized expectations that
one can use to improve proposed solutions. The series representation makes
it possible to organize the calculation around computing a deterministic
problem at time t given a proposed solution.  The deterministic solution
can accommodate inequality constraints or alternative regimes to produce a
solution for each set of initial conditions.  One can typically arrange,
perhaps by adding auxiliary variables, to produce a ``decision rule''
that one can correctly integrate to get a deterministic conditional
expectation function that can be iterated forward and serves to
improve upon the original proposed solution.
Time invariant stochastic functions 
lead naturally to an associated family of deterministic maps
which can conveniently represented by the series representation.





\section{A New Series Representation For Stochastic Model Solution Paths}
\label{sec:newseries}

\subsection{A Linear Reference Model and a Formula for  ``Anticipated Shocks''}
\label{sec:linref}
\begin{itemize}
\item Almost Arbitrary
\item Impact of Choice
\item represents impact of ``anticipated shocks'' interpretation error bounds on $x_t$
\item any bounded path
\end{itemize}




For any linear homogeneous 
$L$ dimensional 
deterministic 
system 
\begin{gather}
  	 H_{-1} x_{t-1} + H_0 x_t + H_1 x_{t+1}=0\label{hSystem}
\end{gather}
that produces  a unique stable solution, 
it is well known\ \citep{anderson10} that  inhomogeneous solutions 
\begin{gather}
	 H_{-1} x_{t-1} + H_0 x_t + H_1 x_{t+1}=\psi_\epsilon \epsilon +\psi_{c}
\intertext{ can be computed as}
x_t=B x_{t-1} + \phi \psi_\epsilon \epsilon + (I - F)^{-1} \phi \psi_c
\intertext{where}
\phi= (H_0 +H_1 B)^{-1}  \text{ and } \,\,F=-\phi H_1 
\end{gather}
It will be useful to collect the components of this representation for use in
subsequent sections of the paper.
Define $\linMod \equiv \linModMats$.


{\small
Now, given the trajectories \refeq{fFamily}, define 
$  z_{t}(x_{t-1},\epsilon)$ as  %\footnote{These $z$ functions will soon prove useful in an algorithm for computing unknown trajectories like \refeq{fFamily}.}:
{
  \begin{align}
  z_{t}(x_{t-1},\epsilon) \equiv& H_{-1} \mathcal{X}_{t-1}(x_{t-1},\epsilon) + \nonumber\\
& H_0 \mathcal{X}_{t}(x_{t-1},\epsilon) +  \label{defZ} \\
& H_1 \mathcal{X}_{t+1}(x_{t-1},\epsilon). \nonumber
  \end{align}
}
\newtheorem{theorem}{Theorem}[section]


\begin{theorem}
	 \begin{gather}
	 \mathcal{X}_{t}(x_{t-1},\epsilon) =B x_{t-1}+ \phi \psi_\epsilon\epsilon + (I - F)^{-1} \phi \psi_c + \sum_{\sForSum=0}^\infty F^s \phi z_{t+\sForSum}(x_{t-1},\epsilon) \label{theSeries}
\intertext{and}
	 \mathcal{X}_{t+1}(x_{t-1},\epsilon) =B \mathcal{X}_{t} + \sum_{\sForSum =0}^\infty F^\sForSum \phi z_{t+\sForSum}(x_{t-1},\epsilon) + (I - F)^{-1} \phi \psi_c \,\,\,\forall t \ge  0.
	 \end{gather}
\end{theorem}
}
\begin{myProof}
  For every bounded family of functions,
  $\mathcal{X}_t(x_{t-1},\epsilon_t)$, \citep{anderson10}  shows that for any given $x_{t-1},\epsilon_t$ this expression must be true.
Since the eigenvalues of F are all less than one and the $z$'s are bounded,
 the series always converges.
\end{myProof}


	 Consequently, given a family of trajectories like those in \refeq{fFamily},
and a stable linear homogeneous system like \refeq{hSystem},
one can easily compute a series 
representation \refeq{theSeries} for any  bounded function generating a family of
trajectories.
Interestingly, the linear model, $H$, the  constant term $\psi_c$ and the
impact of the stochastic shocks $\psi_\epsilon $ can  be 
chosen rather arbitrarily -- the only constraint being the existence of a saddle-point solution or the linear system.  
The formula will provide a series  for any 
$L$ dimensional $\linMod$.
This observation will give us some confidence in the 
robustness of the algorithms described in section 
\ref{sec:unknown-solutions} for constructing series 
representations for unknown families of functions 
satisfying complicated systems of dynamic non-linear equations.




\subsection{A Series Representation for Bounded Paths}
\label{sec:boundedpaths}

\subsection{A Simple Example: An ``Almost'' Arbitrary Linear Model and an Arbitrary Family of Bounded Solution Paths}
\label{sec:almostarbitrary}


Consider the following constructed from ``almost'' arbitrary coefficients
\begin{gather}
  \begin{bmatrix}
H_{-1}&H_{0}&H_{1} 
  \end{bmatrix}=
\vcenter{\hbox{\includegraphics{refHmat.pdf}}}\intertext{with $\psi_c=\psi_\epsilon=0, \,\,  \psi_z=I$.
These coefficients are not completely arbitrary in so far as the series 
representation requires that the linear model
has a unique stable solution.}
  B=
\vcenter{\hbox{\includegraphics{refBmat.pdf}}}\\
\phi=
\vcenter{\hbox{\includegraphics{refPhimat.pdf}}}\\
F=
\vcenter{\hbox{\includegraphics{refFmat.pdf}}}
\end{gather} 



The series representation requires that the state values along the 
paths remain bounded.  For example, consider the following three
bounded families of state vector paths generated by Mathematica code.
\begin{verbatim}
genPiPath[init_?MatrixQ,eps_?MatrixQ,kk_Integer]:=
		With[{theDigits=RealDigits[Pi,10,kk][[1]]},
			 N[(Norm @ init)]/(1+N[Norm @ eps]) *theDigits]
genOscilPath[init_?MatrixQ,eps_?MatrixQ,kk_Integer]:=
		With[{theOscil=NestList[-1*#&,1,kk-1]},
			 N[(Norm @ init)]/(1+N[Norm @ eps])^2 *theOscil]
genRandomPath[init_?MatrixQ,eps_?MatrixQ,kk_Integer]:=
		Module[{},SeedRandom[Round[200*N[(Norm @ init)]+Norm[eps]]]; 
				  RandomReal[{-4,4},kk]]
\end{verbatim}


The first set of trajectories(See Figure \ref{pipath}) is a function of
the digits in the decimal representation of $\pi$.  
determined by a nonlinear function of the initial conditions and a random shock.
The second set of trajectories (See Figure \ref{oscillpath}) oscillates between two values
determined by  a nonlinear function of the initial conditions and the shock.
The third set of trajectories (See Figure \ref{pseudopath}) is a sequence of uniformly distributed random
numbers based on a seed determined by  a nonlinear function of  the initial conditions and the shock.
These examples paths were chosen to emphasize that the trajectories
 need not converge to a fixed point, and 
need not be produced by iteration of a discrete-time map.
The boundedness of the paths is a sufficient condition for the existence 
of the series representation.\footnote{Although useful in some contexts,
this paper will not investigate sufficient conditions for families of
unbounded trajectories.}


\begin{figure}
  \centering
\includegraphics[width=2in]{piPath.pdf}
  
  \caption{$\pi$ Digits Path}\label{pipath}
\end{figure}
\begin{figure}
  \centering
\includegraphics[width=2in]{oscillPath.pdf}
  
  \caption{Oscillatory Path}\label{oscillpath}
\end{figure}

\begin{figure}
  \centering
\includegraphics[width=2in]{pseudoPath.pdf}
  \caption{Uniformly Distributed Path}\label{pseudopath}
\end{figure}

\begin{figure}
  \centering
\includegraphics[width=3in]{theZs.pdf}  
  \caption{The  z's Corresponding to  $x_{-1}=(1,2,3),\epsilon=(2,1,2)$} \label{arbFig}
\end{figure}

\begin{figure}
  \centering


\includegraphics[width=3in]{arbTruncErr.pdf}  
  \caption{Error Bound Versus Actual Error} \label{figArbTrunc}

\end{figure}


Figure \ref{arbFig} shows, for a particular initial state vector and shock value,  the paths for the state vectors and the correct z's that generate the path.
One can repeat the calculations for any given initial condition to produce
a z series exactly replicating the given set of trajectories.  The family
of z functions along with equation \ref{theSeries} provide a series 
representation for the family of trajectories.  Figure \ref{figArbTrunc} shows
that the truncation error bound is a very conservative measure of the accuracy
of the truncated series.  The series requires only the first 20 terms to compute
the initial value of the state vector to machine precision. 
The series representation can compute the entire series to machine precision
if all the terms are included, but the terms for state vectors closer 
to the initial time have the most important impact.





\subsubsection{Assessing $x_t$ Error}
\label{sec:truncationerr}
\paragraph{Truncation Error}


One could consider approximating $\mathcal{X}_t(x_{t-1},\epsilon)$ by 
truncating the series \ref{theSeries} at a finite number of terms.
 	 \begin{gather}
 	 \xWargK \equiv B x_{t-1}+ \phi \psi_\epsilon\epsilon + \sum_{s=0}^k F^s \phi z_{t}(x_{t-1},\epsilon) + (I - F)^{-1} \phi \psi_c \label{theTruncSeries}
 \end{gather}
We can bound the  series approximation truncation errors.
Since
    \begin{gather}
      \label{eq:1}
\sum_{s=k+1}^{\infty} F^s \phi \psi_z = (I -F)^{-1} F^{k+1}\phi \psi_z       \\
\infNorm{\xWarg-\xWargK} \le \infNorm{(I -F)^{-1} F^{k+1}\phi \psi_z} \left ( \infNorm{H_{-1} }+ \infNorm{H_{0} }+ \infNorm{H_{1} } \right )\bar{\mathcal{X}}
    \end{gather}
Note that for approximating $\xWarg$ the impact of  a given realization along the path declines for those realizations which are  more temporally distant.




\paragraph{Pessimistic Error}

\subsubsection{A Path Norm}
\label{sec:pathnorm}

\section{Nonlinear Dynamic Stochastic Time Invariant Maps}
\subsection{Application to Time Invariant Maps}
\label{sec:extToMaps}

\subsection{Auxiliary Variable Reformulation}
\label{sec:aux}



\subsubsection{Dynamic Stochastic Models, Model Solutions  and Conditional Expectations}
\label{sec:condExp}

\begin{itemize}
\item iterated expectations $E (x | φ1) = E (E (x | φ2) | φ1) $
  \href{https://stats.stackexchange.com/questions/95947/a-generalization-of-the-law-of-iterated-expectations}{a proof}
\item will want to apply the law of iterated expectations so we will construct aixiliary variables that embed any nonlinear combinanation  of the orignal
  variables that might be requred to make this work.  Although it is possible to construct models where this may be difficult, many important economic models
  can easily be adapted this way.

\end{itemize}


\subsubsection{RBC Example General Formulation}
\label{sec:rbcaux}

\begin{itemize}
\item composition in effect imposes for number of periods
\end{itemize}



To describe the algorithm we will, for the moment, omit inequality constraints
and regime switching.
We will develop an algorithm for finding solutions for models that can be written in  the form


\begin{gather}
  h_i(x_{t-1},x_{t},x_{t+1},\epsilon_t)=h^{det}_{io}(x_{t-1},x_{t},\epsilon_t)+\sum_{j=1}^{p_i} [h^{det}_{ij}(x_{t-1},x_{t},\epsilon_t)h^{nondet}_{ij}(x_{t+1})]=0
\end{gather}
This is a very broad class of models including most widely used
macroeconomics models.

For example, the Euler equations for the  neoclassical growth  model 
\label{sec:simple-rbc-model-ext} can be written as
\begin{gather}
h_{10^{det}}(\cdot)=\frac{1}{c_t},\,\,
h_{11}^{det}()=\alpha \delta k_{t}^{\alpha-1} ,\,\,
h_{11}^{nondet}(\cdot)=E_t \left (\frac{\theta_{t+1}}{c_{t+1}} \right )\\
h_{20}^{det}(\cdot)=c_t + k_t-\theta_tk_{t-1}^\alpha,\,\,
h_{21}^{det}(\cdot)=0\\
h_{30}^{det}(\cdot)=\ln \theta_t -(\rho \ln \theta_{t-1} + \epsilon_t),\,\,
h_{31}^{det}(\cdot)=0
\end{gather}
Since we will be working with models where expectations are computed at time t, $\epsilon_t$ is known.  The only stochastic components are those with time subscripts greater than $t$. Since we will need to compute 
the conditional expectation of nonlinear expressions,  
this setup will make it possible for us to use auxiliary
variables to correctly compute the required expected values.
Below, we will consider 
systems that augment these dynamic equations with additional constraints 
on the evolution of the variables.


The algorithm will construct
 a time invariant function $g^\ast$ that satisfies
\begin{gather}
  \begin{split}
h(x_{t+s-1},g^\ast(x_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(x_{t+s-1},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) \label{theProblem} \\
%m(x_{t+s-1},g^\ast(x_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(x_{t+s-1},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) \ge 0  
  \end{split}
%  \intertext{define} 
% \mathcal{G}^\ast(x_{t+s-1},\epsilon_{t+s})= \mathcal{H}[g^\ast(g^\ast(x_{t+s-1},\epsilon_{t+s}),\epsilon_{t+s+1})] \nonumber
 \end{gather}
 for all $s>0$ where $\mathcal{H}$ is an operator, 
  that maps stochastic functions of $x$ and $\epsilon$ into deterministic 
functions of $x$ alone.  In this paper we will consider two such operators:










We begin by choosing some linear model of appropriate dimension that 
has a uniquely convergent steady state.  
Although this is not necessary, it may be possible to obtain this model by
linearizing the original model around the deterministic steady state.\footnote{As noted above, one could construct a series representation using any linear
 model of appropriate dimension with a unique stable solution.  The rate of convergence and the number of terms required for a  given level of accuracy will depend upon the linear model employed.}


 \begin{gather}
 g^0(x_{t-1},\epsilon_{t})=  
B x_{t-1}+ \phi \psi_\epsilon\epsilon_{t} +
 (I - F)^{-1} \phi \psi_c\\ \label{firstIter}
z^{0}_{t+i}(x_{t-1},\epsilon_{t-1})=0 \,\, \forall i \ge 0
 \end{gather}
We are now in a position to easily compute 
 \begin{gather}
\xIter{0}{x}=\mathcal{H}^{PF}[g^{0}(x,\epsilon_{t-k+1})]=
\mathcal{H}^{RE}[g^{0}(x,\epsilon_{t-k+1})]= 
B x+  (I - F)^{-1} \phi \psi_c
 \end{gather}
\begin{gather}
\underbrace{x_{t-1},0} 
\underbrace{\xNow{0},\zNow{0}}
\underbrace{\xNowtp{0}=\xIter{0}{x_{t}},0}
\end{gather}
In other words, from time t onwards, this approximation assumes 
that our stand-in
 convergent linear model completely characterizes the solution.\footnote{
The algorithm terminates with an approximation for 
the time invariant function $g^\ast$.}  It is unlikely that $g^0(x_{t-1},\epsilon_{t})$
satisfies equation \refeq{theProblem} for arbitrary $x_{t-1}$ even for $s=0$.
We can get a pessimistic upper bound for how far we are away from the solution 
by computing the infinity norm of substituting the variables from into Equation \refeq{theProblem}.
% \footnote{The values in Table \ref{truncTab} illustrate how
%pessimistic this approximation can be, but it turns out we can expect the accur%acy of  approximate truncation errors improve as we expand the approximation series.}

Next, we compute a solution in which the nonlinear equations are satisfied at time t alone while the stand-in linear system holds sway subsequently.
Our algorithm will  compute
\begin{gather}
\underbrace{x_{t-1},0} 
\underbrace{\xNow{1},\zNow{1}}
\underbrace{\xNowtp{1}=\xIter{1}{x_{t+1}},0}
\underbrace{\xNowtp{1}=\xIter{0}{x_{t+1}},0}
\end{gather}
that satisfy the nonlinear equation system at time t.
For a given $x_{t-1}$, we substitute
\begin{gather}
x_{t-1}, \,\,
  x_t=B x_{t-1} + \phi z^1(x_{t-1},\epsilon_t) , \,\,
  x_{t+1}=B x_{t} + \phi \mathcal{Z}^1(x_t)
\end{gather}
into the non linear equation system to determine the 
$z^1(x_{t-1},\epsilon_t)$ and consequently $x_t^1(x_{t-1},\epsilon_t)$.
We then have
 \begin{gather}
 g^1(x_{t-1},\epsilon_{t})=  
B x_{t-1}+ \phi \psi_\epsilon\epsilon_{t} +
\phi \psi_\epsilon z^1(x_{t-1},\epsilon_t)+
 (I - F)^{-1} \phi \psi_c\\ \label{firstIter}
z^{1}_{t+i}(x_{t-1},\epsilon_{t-1})=0 \,\, \forall i \ge 1
 \end{gather}
Now, $g^1(x_{t-1},\epsilon_{t})$
satisfies equation \refeq{theProblem} for arbitrary $x_{t-1}$ at time $t$ with $s=0$.  However, it probably does not solve the system for $s>0$.
We can get a pessimistic upper bound for how far we are away from the solution 
by substituting the variables and computing the infinity norm for plausible values of $x_{t-1}$.
\begin{gather}
\xNow{1},\xNowtp{1},\xNowtp{1}=\xIter{1}{x_{t+1}}
\end{gather}
into Equation \refeq{theProblem}. If the accuracy of the approximation is adequate, we need not compute any further  terms in the power series.  If not,
we are now in a position to compute our choice of
$\mathcal{Z}^1(x)=\mathcal{H}^{PF}[g^{1}(x,\epsilon_{t-k+1})]$ or
$\mathcal{Z}^1(x)=\mathcal{H}^{RE}[g^{1}(x,\epsilon_{t-k+1})]$.

%  \begin{gather}
%  g^0(x_{t-1},\epsilon_{t})=  
% B x_{t-1}+ \phi \psi_\epsilon\epsilon_{t} +
%  (I - F)^{-1} \phi \psi_c\\ \label{firstIter}
% z^{0}_{t+i}(x_{t-1},\epsilon_{t-1})=0 \,\, \forall i \ge 0
%  \end{gather}
%  \begin{gather}
%  g^0(x_{t-1},\epsilon_{t})=  
% B x_{t-1}+ \phi \psi_\epsilon\epsilon_{t} +
%  (I - F)^{-1} \phi \psi_c\\ \label{firstIter}
% z^{0}_{t+i}(x_{t-1},\epsilon_{t-1})=0 \,\, \forall i \ge 0
%  \end{gather}

Using these functions we can then compute a solution for a model whose trajectories satisfy equation system \refeq{rbcSys}
for two time periods and satisfy equation \refeq{rbcLinSys} subsequently.

\begin{gather}
\underbrace{x_{t-1},0}
\underbrace{\xNow{2},\zNow{2}} 
\underbrace{\xNowtp{2}=\XNow{2}{t}{t},\ZNow{2}{1}{t}}
\underbrace{\XNow{1}{t+1}{t},0}\underbrace{\XNow{0}{t+2}{t+1},0}    \intertext{where we have solved for $\xNow{2}$ and $\zNow{2}$ to satisfy the  equation system \refeq{theProblem}.}
\xNowtp{2}=\XNow{1}{t}{t}
\end{gather}

\begin{gather}
  \label{eq:1}
  x_t=B x_{t-1} + \phi z^2(x_{t-1},\epsilon) + F \phi \mathcal{Z}^1(x_t)\\
  x_{t+1}=B x_{t} + \phi \mathcal{Z}^1(x_t)+
 (I - F)^{-1} \phi \psi_c\\
  x_{t+2}=B x_{t+1} + (I - F)^{-1} \phi \psi_c\\
  x_{t+3}=B x_{t+2} + (I - F)^{-1} \phi \psi_c
\end{gather}

The next iteration highlights an aspect of the algorithmic step which allows us
to continue to exploit the series representation.

{\small
\begin{gather}
\underbrace{x_{t-1},0}
\underbrace{\xNow{3},\zNow{3}} 
\underbrace{\xNowtp{3}=\XNow{3}{t}{t},\ZNow{3}{1}{t}} 
\underbrace{\XNow{2}{t+1}{t},\ZNow{2}{2}{t+1}} 
\underbrace{\XNow{1}{t+3}{t+2},0}
\underbrace{\XNow{0}{t+4}{t+3},0}
\end{gather}
}

\begin{gather}
  \label{eq:1}
  x_t=B x_{t-1} + \phi z^3(x_{t-1},\epsilon) + F \phi \mathcal{Z}^2(x_t)+ F^2 \phi \mathcal{Z}^1(x_t)+
 (I - F)^{-1} \phi \psi_c\\
  x_{t+1}=B x_{t} + \phi \mathcal{Z}^1(x_t)+ F^2 \phi \mathcal{Z}^1(x_t)+
 (I - F)^{-1} \phi \psi_c\\
  x_{t+2}=B x_{t+1} + (I - F)^{-1} \phi \psi_c\\
  x_{t+3}=B x_{t+2} + (I - F)^{-1} \phi \psi_c\\
  x_{t+4}=B x_{t+2} + (I - F)^{-1} \phi \psi_c
\end{gather}



In the general iteration step we will have
\begin{gather}
  \label{eq:1}
  x_t=B x_{t-1}  \phi \psi_\epsilon \epsilon_t + (I - F)^{-1} \phi \psi_c +
\sum_{s=0}^{k} F^s \phi \mathcal{Z}^{k-s}(x_{t+s}^{k+1})\\
  x_{t+s}^{k+1}=\Pi_{s=0}^k \xIter{s}{ x_{t}^{k+1}}
\end{gather}



Needs generalization of ``chunk'' $\frac{1}{c_t}\rightarrow \left (\frac{\theta_{t+1}}{c_{t+1}^\eta} \right )$

We construct our stand-in model by augmenting the model with the equation
\begin{gather}
  \rcpC_t=\frac{1}{c_t}
\end{gather}
substituting $\rcpC_{t+1}$ for $\frac{1}{c_{t+1}}$ in the first equation and 
 linearizing the RBC model about the ergodic mean
given in \refeq{rbcparams}
{\small
\begin{gather}
  \begin{bmatrix}
H_{-1}&H_{0}&H_{1} 
  \end{bmatrix}=\\
\vcenter{\hbox{\includegraphics{RBCHmatSymb.pdf}}} \label{rbcLinSys}
\intertext{with}
\psi_\epsilon=
\begin{bmatrix}
  0\\0\\1\\0
\end{bmatrix}, \psi_z=I
\end{gather}%(\footnote{generated by AMAPaperCalcs.mth {RBCHmatSymb.pdf}})
}
These coefficients  produce a unique stable linear solution.

\begin{gather}
  B=
\vcenter{\hbox{\includegraphics{RBCBmatSymb.pdf}}},
\phi=
\vcenter{\hbox{\includegraphics{RBCPhimatSymb.pdf}}}\\
F=
\vcenter{\hbox{\includegraphics{RBCFmatSymb.pdf}}}\\
\psi_c=
\vcenter{\hbox{\includegraphics{RBCHSum.pdf}}}
\vcenter{\hbox{\includegraphics{RBCSS.pdf}}}=\vcenter{\hbox{\includegraphics{RBCPsissSymb.pdf}}}
\end{gather}%(\footnote{generated by AMAPaperCalcs.mth {RBCBmatSymb.pdf}})(\footnote{generated by AMAPaperCalcs.mth {RBCPhimatSymb.pdf}})(\footnote{generated by AMAPaperCalcs.mth {RBCFmatSymb.pdf}})(\footnote{generated by AMAPaperCalcs.mth {RBCHSum.pdf}})(\footnote{generated by AMAPaperCalcs.mth {RBCSS.pdf}})

Applying the formula \refeq{firstIter} produces:

{\tiny
\begin{gather}
  \begin{bmatrix}
c_t\\k_t\\ \rcpC_t\\\theta_t
  \end{bmatrix}=%paperCalcsRBCExample xt00
   \left(
   \begin{array}{c}
 0.359845 \epsilon _t+0.692632 k_{t-1}+0.341853 \theta _{t-1}-0.0442851
   \text{z1}_{t-1}+0.658 \text{z2}_{t-1}+0.359845 \text{z3}_{t-1}-0.111552 \\
 0.187032 \epsilon _t+0.36 k_{t-1}+0.17768 \theta _{t-1}+0.0442851
   \text{z1}_{t-1}+0.342 \text{z2}_{t-1}+0.187032 \text{z3}_{t-1}-0.0579799 \\
 -5.34898 k_{t-1}+0.342 \text{z1}_{t-1}-5.08153
   \text{z2}_{t-1}+\text{z4}_{t-1}+3.7794 \\
 \epsilon _t+0.95 \theta _{t-1}+\text{z3}_{t-1}+0.05 \\
   \end{array}
   \right)
\end{gather}
}

and 


{\tiny
%xt01=Private`computeNextXt[{Private`bmat,Private`phimat,Private`fmat,Private`psieps,Private`psic,Private`psiz},solnFunc00PF[[3+Range[3]]],{{cc},{kk},{tt}},{1}]//N//Expand//Simplify
\begin{gather}
  \begin{bmatrix}
c_{t+1}\\k_{t+1}\\ \rcpC_{t+1}\\\theta_{t+1}
  \end{bmatrix}=%paperCalcsRBCExample xt00
  \left(
   \begin{array}{c}
 0.471397 \epsilon _t+0.249347 k_{t-1}+0.447827 \theta _{t-1}+0.0306732
   \text{z1}_{t-1}+0.23688 \text{z2}_{t-1}+0.471397 \text{z3}_{t-1}-0.134618 \\
 0.245012 \epsilon_t+0.1296 k_{t-1}+0.232761 \theta _{t-1}+0.0159426
   \text{z1}_{t-1}+0.12312 \text{z2}_{t-1}+0.245012 \text{z3}_{t-1}-0.0699687 \\
 -1.00043 \epsilon _t-1.92563 k_{t-1}-0.950409 \theta _{t-1}-0.23688
   \text{z1}_{t-1}-1.82935 \text{z2}_{t-1}-1.00043 \text{z3}_{t-1}+4.08954 \\
 0.95 \epsilon _t+0.9025 \theta _{t-1}+0.95 \text{z3}_{t-1}+0.0975 \\
   \end{array}
   \right)
\end{gather}}

Substituting  these expressions into equation \refeq{rbcSys} produces
a deterministic system such that, given specific values for 
$(x_{t-1},\epsilon_{t})=(c_{t-1}, k_{t-1},\rcpC_{t-1}, \theta_{t-1}, \epsilon_t)$, we can solve for $z_{1t}(x_{t-1},\epsilon_{t})$, $z_{2t}(x_{t-1},\epsilon_{t})$, $z_{2t}(x_{t-1},\epsilon_{t})$, and $z_{4t}(x_{t-1},\epsilon_{t})$  completely determining
$c_{t}(x_{t-1},\epsilon_{t})$, $k_{t}(x_{t-1},\epsilon_{t})$, $\rcpC_{t}(x_{t-1},\epsilon_{t})$  and $\theta_{t}(x_{t-1},\epsilon_{t})$.\footnote{In this example, the lagged value,  $c_{t-1}$ does not appear in the equation system and consequently plays no role in determining the solution.}  In effect we have 
computed a solution for a model whose trajectories satisfy equation system \refeq{rbcSys}
for one time period and satisfy equation \refeq{rbcLinSys} subsequently.

We are now in a position to compute
$\mathcal{H}^{PF}[g^{0}(x,\epsilon_{t-k+1})]$ or
$\mathcal{H}^{RE}[g^{0}(x,\epsilon_{t-k+1})]$.
Using these functions we can then compute a solution for a model whose trajectories satisfy equation system \refeq{rbcSys}
for two time periods and satisfy equation \refeq{rbcLinSys} subsequently.
\begin{gather}
  \label{eq:1}
  x_t=B x_{t-1} + \phi z_0(x_{t-1},\epsilon) + F \phi \mathcal{Z}^1(x_t)\\
  x_{t+1}=B x_{t} + \phi \mathcal{Z}^1(x_t)
\end{gather}

We will see that all these can precomputed.
\subsubsection{General Formulation}
\label{sec:genAux}
  {Discovering Unknown Solutions}

A, conceptually, very simple solution strategy:  

  \begin{enumerate}
  \item Begin with some convergent linear model, $\linMod$, of appropriate dimension.
  \item Compute solutions honoring the model equations for time t, 
but that assume the trajectories subsequently 
evolve according to the convergent 
linear model, $\linMod$.
  \item Given solutions valid for time $t$ to $t+k$, extend the solutions to be valid for time $t$ to $t+k+1$ \label{stepNo}
\item Repeat step \ref{stepNo} unless truncation formulas indicate the solution is sufficiently accurate
\item collection of equations systems with logic gate.
  \begin{itemize}
  \item mutually exclusive
  \item exhaustive
  \end{itemize}
  \end{enumerate}






  {Algorithm based on basic properties of deterministic maps.}
  \begin{itemize}
  \item     The $\epsilon$'s  play two distinct roles in the algorithm.
To see the two roles, consider a family of stochastic functions 
$\{\stochF{T},\ldots,\stochF{0}\}$ with 
$\stochF{k}(x,\epsilon_t): {\mathcal{R}}^{L+1} \rightarrow \mathcal{R}^L$ 
    \begin{enumerate}
\item I assume  $\epsilon_t \sim $ iid. Knowing the distribution 
of the $\epsilon_t$ makes it possible to compute 
a corresponding family of deterministic functions $\detF{k}(x)=E( \stochF{k}(x,\epsilon_t)|x) $
    \item I assume that at some initial time, say t=0, a particular 
 $\epsilon_0$ draw 
along with an $x_{-1}$, determines a unique $x_0=\stochF{T}\initXE$.
Subsequently, $x_{t}=\detF{{T-t}}(x_{t-1})\,\, \forall t>0$
    \end{enumerate}
 \item The series representation requires a unique trajectory beginning with $x_0=\stochF{T}\initXE$ and 
evolving forward from any $x_0$ given by, a potentially time varying deterministic map,  $x_t=\detF{T-t}(x_{t-1})\,\, \forall t>0$. 
  \item The algorithm for discovering unknown solutions does not rely on time invariance.
  \end{itemize}
    






  {Recursive updating}
{\small
  \begin{itemize}
  \item Given a linear reference model,$\linModMats$,  any bounded 
sequence of deterministic maps $\{\detF{T},\ldots,\detF{0}\}, \,\, T>0$ generates 
a series representation
  \begin{gather}
     \label{eq:2}
	 \mathcal{X}_{t}(x_{t-1}) =B x_{t-1}+  (I - F)^{-1} \phi \psi_c +\\ \sum_{\sForSum=0}^{T-1} F^s \phi z_{T-\sForSum}(x_{t-1}) \intertext{it is straightforward to 
update to a new series representation for the map trajectories resulting from prepending an additional deterministic map to the sequence $\detF{T+1},\ldots,\detF{0}\}$}
	 \mathcal{X}_{t}(x_{t-1}) =B x_{t-1}+  (I - F)^{-1} \phi \psi_c +\\ 
\phi z_{T+1}(x_{t-1}) + \sum_{\sForSum=1}^{T} F^s \phi z_{T+1-\sForSum}(x_{t-1}) 
  \end{gather}
\item 
  \end{itemize}
}



%\footnote{We set $\psi_c=0$  without loss of generality 
%in the expressions that follow merely to save on notation.}


{The model of interest and a linear reference model}
\begin{itemize}
\item The target model is of the form
\begin{gather}
\nlEqnLHS{x_{t-1}}{x_t}{x_{t+1}}{t}=\\
\nlEqn{x_{t-1}}{x_t}{x_{t+1}}{t}=0\\ \label{refMod}
 (L \times 1) +  (L \times L ) \cdot (L \times 1)\\
\nlEqnSel{x_{t-1}}{x_t}{x_{t+1}}{t}\\ 
\end{gather}



\item The algorithm constructs a sequence of stochastic 
functions 
  \begin{gather}
	 \stochF{k}\initXE =B x_{t-1}+ (I - F)^{-1} \phi \psi_c+  \phi \psi_\epsilon\epsilon_0  +\\ \phi z^{\{k\}}\initXE+ \sum_{\sForSum=1}^{k-1} F^s \phi z^{\{k-\nu\}}_{\sForSum}\initXE 
\label{theSeries}
  \end{gather}

\item One can show that the algorithm begins with solutions satisfying 
the linear reference model $\linModMats$ and
constructs a sequence of functions
 honoring the target model equations for successively 
longer solution horizons.
  \end{itemize}




  {Correctly Computing Expectations of Nonlinear Functions}

  \begin{itemize}
\item Auxiliary equations for non linear ``chunks'' provide mechanism for characterizing  nonlinear expectations in the future
  \item Formula maps future equation errors  to current time changes in model
variables.
\item  $\epsilon_t$ is known so that,
given a function characterizing future expectation,  model requires
a deterministic solution at time $t$
\item The formula is linear in the potentially nonlinear $z$ functions
  \end{itemize}




{Early iterations}
{\small
  \begin{itemize}
  \item $k=0$: Completely ignore the target model
  \begin{gather}
z^{\itSup{0}}\initXE=0 \intertext{ so that}
\stochF{0}\initXE=B x_{-1} + (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0  \intertext{initialize the sequence of deterministic maps with }
\{\detF{0}\initX=B x \}
  \end{gather}
  \item $k=1$: Employ the target model for one period. Use
  \begin{gather}
x_0\initXE=B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + \phi z^{\itSup{1}}\initXE\\
E(x_1\initXE)=\detF{0}(x_0\initXE) \intertext{in the deterministic equation 
\refeq{refMod} to determine $z^{\itSup{1}}\initXE$ so}
\stochF{1}\initXE = B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + 
\phi z^{\itSup{1}}\initXE \intertext{%Define $Z^{\itSup{1}}(x)\equiv E \left [ z^{\itSup{1}}\initXEN | x \right ]$ and 
 augment the sequence}
\{\detF{1}\initX=B x  + \phi Z^{\itSup{1}}(x),\detF{0}\}
  \end{gather}
  \end{itemize}
}



{Early iterations}
{\small
  \begin{itemize}
  \item $k=2$: Employ the target model for two periods. Use
  \begin{gather}
x_0\initXE=B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 +\\ \phi z^{\itSup{2}}\initXE + F \phi Z^{\itSup{1}}(x_0\initXE)\\
E(x_1\initXE)=\detF{1}(x_0\initXE) \intertext{in the deterministic equation 
\refeq{refMod} to determine $z^{\itSup{2}}\initXE$}
\stochF{2}\initXE = B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 +\\ \phi z^{\itSup{2}}\initXE + F \phi Z^{\itSup{1}}(x_0\initXE)\intertext{
Define $Z^{\itSup{2}}(x)\equiv 
E \left [ z^{\itSup{2}}\initXEN | x \right ]$}
\{\detF{2}\initX=B x  + \phi  Z^{\itSup{2}}(x) + F \phi Z^{\itSup{1}}(x_0\initXE),
\detF{1}\initX,\detF{0}\initX\}
  \end{gather}
  \end{itemize}
}




{Early iterations}
{\small
  \begin{itemize}
  \item $k=3$: Employ the target model for two periods. Use
  \begin{gather}
x_0\initXE=B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + \phi z^{\itSup{3}}\initXE +\\ F \phi Z^{\itSup{2}}(x_0\initXE) + F^2 \phi Z^{\itSup{1}}(\detF{2}(x_0\initXE))\\
E(x_1\initXE)=\detF{2}(x_0\initXE) \intertext{in the deterministic equation 
\refeq{refMod} to determine $z^{\itSup{3}}\initXE$}
\stochF{3}\initXE = B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + \phi z^{\itSup{3}}\initXE + \\F \phi Z^{\itSup{2}}(x_0\initXE) +F^2 \phi Z^{\itSup{1}}(\detF{2}(x_0\initXE))\intertext{
Define $Z^{\itSup{3}}(x)\equiv 
E \left [ z^{\itSup{3}}\initXEN | x \right ]$}
\detF{2}\initX=B x  + \phi  Z^{\itSup{3}}(x) + 
F \phi Z^{\itSup{2}}(x_0\initXE) + \\F^2 \phi Z^{\itSup{1}}(\detF{1}(x_0\initXE))
  \end{gather}
  \end{itemize}
}



{General iterations}
{\small
  \begin{itemize}
  \item $k=K+1$: Employ the target model for $K+1$ periods. Use
  \begin{gather}
x_0\initXE=B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + \phi z^{\itSup{K+1}}\initXE +\\ \sum_\sForSum^K F^\nu \phi Z^{\itSup{K+1-s}}\detComp{x_0\initXE} \\
E(x_1\initXE)=\detF{K}(x_0\initXE) \intertext{in the deterministic equation 
\refeq{refMod} to determine $z^{\itSup{K+1}}\initXE$}
\stochF{K+1}\initXE = B x_{-1}+ (I - F)^{-1} \phi \psi_c +\phi \psi_\epsilon \epsilon_0 + \phi z^{\itSup{K+1}}\initXE + \\
\sum_\sForSum^K F^\nu \phi Z^{\itSup{K+1-s}}\detComp{x_0\initXE} 
  \end{gather}
  \end{itemize}
}








\subsubsection{An RBC Model Example}
\label{sec:rbcexample}

\subsection{Example: A Simple RBC Model Example With Known Solution}
\label{sec:simple-rbc-model-2}


  {The RBC Model ($\eta=1$): Recovering a Known Solution}
  \begin{itemize}
  \item Compute $\linMod$ for RBC Model $\eta=1,\rcpC_t=\frac{1}{c_t}$
  \end{itemize}

Applying formula \refeq{theSeries} produces:

{\tiny
\begin{gather}
  \begin{bmatrix}
c_t\\k_t\\ \rcpC_t\\\theta_t
  \end{bmatrix}=\\%paperCalcsRBCExample xt00
   \left(
   \begin{array}{c}
 0.359845 \epsilon _t+0.692632 k_{t-1}+0.341853 \theta _{t-1}-0.0442851
   \text{z1}_{t-1}+0.658 \text{z2}_{t-1}+0.359845 \text{z3}_{t-1}-0.111552 \\
 0.187032 \epsilon _t+0.36 k_{t-1}+0.17768 \theta _{t-1}+0.0442851
   \text{z1}_{t-1}+0.342 \text{z2}_{t-1}+0.187032 \text{z3}_{t-1}-0.0579799 \\
 -5.34898 k_{t-1}+0.342 \text{z1}_{t-1}-5.08153
   \text{z2}_{t-1}+\text{z4}_{t-1}+3.7794 \\
 \epsilon _t+0.95 \theta _{t-1}+\text{z3}_{t-1}+0.05 \\
   \end{array}
   \right)
\end{gather}
}

and 


{\tiny
%xt01=Private`computeNextXt[{Private`bmat,Private`phimat,Private`fmat,Private`psieps,Private`psic,Private`psiz},solnFunc00PF[[3+Range[3]]],{{cc},{kk},{tt}},{1}]//N//Expand//Simplify
\begin{gather}
  \begin{bmatrix}
c_{t+1}\\k_{t+1}\\ \rcpC_{t+1}\\\theta_{t+1}
  \end{bmatrix}=\\%paperCalcsRBCExample xt00
  \left(
   \begin{array}{c}
 0.471397 \epsilon _t+0.249347 k_{t-1}+0.447827 \theta _{t-1}+0.0306732
   \text{z1}_{t-1}+0.23688 \text{z2}_{t-1}+0.471397 \text{z3}_{t-1}-0.134618 \\
 0.245012 \epsilon_t+0.1296 k_{t-1}+0.232761 \theta _{t-1}+0.0159426
   \text{z1}_{t-1}+0.12312 \text{z2}_{t-1}+0.245012 \text{z3}_{t-1}-0.0699687 \\
 -1.00043 \epsilon _t-1.92563 k_{t-1}-0.950409 \theta _{t-1}-0.23688
   \text{z1}_{t-1}-1.82935 \text{z2}_{t-1}-1.00043 \text{z3}_{t-1}+4.08954 \\
 0.95 \epsilon _t+0.9025 \theta _{t-1}+0.95 \text{z3}_{t-1}+0.0975 \\
   \end{array}
   \right)
\end{gather}}





 {\bf RBC Model Example}
  See \cite{Maliar2005}
 \begin{gather*}
   \max\left \{  u(c_t^t) + E_t \sum_{\tau=t}^\infty \beta \delta^{\tau+1-t}u(c_{\tau+1}^t)\right \}\\
c_\tau^t + k_\tau^{t+1}=(1-d)k_\tau^{t-1} + \theta_\tau f(k_\tau^{t-1})\\
f(k_\tau^{t-1})= k_\tau^\alpha
 \end{gather*}

\begin{gather}
\frac{1}{c_t^{\eta}}=\alpha \delta k_{t}^{\alpha-1} E_t \left (\frac{\theta_{t}}{c_{t+1}^\eta} \right ) \\
c_t + k_t=\theta_{t-1}k_{t-1}^\alpha \\
 \theta_t =\theta_{t-1}^\rho e^{\epsilon_t}\label{rbcSys}
\intertext{for $\eta=1$}
\frac{1}{c_t}=\alpha \delta k_{t}^{\alpha-1} E_t \left (\frac{\theta_{t}}{c_{t+1}} \right ) \\
c_t + k_t=\theta_{t-1}k_{t-1}^\alpha \\
\theta_t =\theta_{t-1}^\rho e^{\epsilon_t}\label{rbcSys}
\intertext{and there is a closed form solution}
  k_{t}= \alpha \delta \theta_{t} k_{t-1}^\alpha.\label{soln}\\
c_t=  (1-\alpha \delta) \theta_{t} k_{t-1}^\alpha
\end{gather}
For mean zero iid $\epsilon_t$ we can easily compute a family of trajectories like \refeq{fFamily}
\begin{gather}
  \begin{bmatrix}
c_{t+s}(k_{t-1},\theta_t,\epsilon_t)\\k_{t+s}(k_{t-1},\theta_t,\epsilon_t)    \\ \theta_{t+s}(\theta_{t-1},\theta_t,\epsilon_t)    
  \end{bmatrix}
\intertext{with conditional mean converging over time to }
  \begin{bmatrix}
    c_{ss}\\k_{ss}
  \end{bmatrix}=
  \begin{bmatrix}
\nu^\alpha-\nu\\ \nu
  \end{bmatrix}\intertext{where}
\nu= \alpha ^{\frac{1}{1-\alpha }} \delta ^{\frac{1}{1-\alpha }}
\end{gather}


We can use the family of conditional expectations
along with the contrived reference model to recover an 
approximation for equation \refeq{soln} along with error bounds.
The series representation provides a weighted sum of z functions that give us
an approximation for the known solution.
\footnote{Note that the reference model is deterministic and the $z$ functions account for the stochastic nature of the model.}
% \footnote{
% We need not  make these adjustments for the steady state,
% but doing so economizes on the number of terms 
% required for a given level of approximation
% accuracy.}

Using the following parameter values

\begin{gather}
\vcenter{\hbox{\includegraphics{../../paperProduction/occBind/docs/RBCParamSubs.pdf}}} \,\, \text{ we have } \,\,
  \begin{bmatrix}
    c_{ss}\\k_{ss} \\ \theta_{ss} \label{rbcparams}
  \end{bmatrix}=
\vcenter{\hbox{\includegraphics{../../paperProduction/occBind/docs/RBCSSVal.pdf}}}
\end{gather}%(\footnote{{../../paperProduction/occBind/docs/RBCParamSubs.pdf}})(\footnote{{RBCSSVal.pdf}})


\begin{figure}
  \centering
\includegraphics{simpBoundsVActual.pdf}  
  \label{rbcTrunc}
  \caption{RBC Truncation Error Bound Versus Actual}
\end{figure}


\subsubsection{Relation to Impulse Response Functions and Stochastic Simulations}
\label{sec:relImp}


Use (Impulse Response Functions?)

Proceeding as though no uncertainty in the parameters.
\href{https://books.google.co.uk/books?id=kLiRDgAAQBAJ&pg=PA77&lpg=PA77&dq=nonlinear+models+impulse+response+function+conditional+expectations&source=bl&ots=1pCjlcgxSu&sig=WA9INrvXVcFhVOd_2wq8-V6alw8&hl=en&sa=X&ved=0ahUKEwid1Y3fvuPWAhUlKsAKHT7lCDkQ6AEITjAF#v=onepage&q=nonlinear%20models%20impulse%20response%20function%20conditional%20expectations&f=false}{book on nonlinear impulse response}
  Potter 1995 2000 and Koop 1996

  from  Elements of Nonlinear Time Series Analysis and Forecasting  Jan G. De Gooijer

CONDITIONAL EXPECTATIONS ASSOCIATED WITH STOCHASTIC PROCESSES
R . A. BROOKS

Law of iterated expectations
\href{http://www.pnas.org/content/103/11/3968.full}{discrete dynamical conditional expectation} 

\subsection{Computing Model Solution Error Bounds}
\label{sec:solnerrorbounds}

We will consider time invariant maps that arise form solving a time t optimization problem codified in a collection of systems of equations.  We will assume that we have a countable collection of equations systems that are mutually exclusive and exhaustive.  Given $\epsilon_t, x_{t-1}$  This system of equations produces a solution $x_t$.  The $x_t$ satisfies one and only one of the collection of equations and$x_t$ is (locally) the unique solution for the collection of
equations.  Examples of such systems include typcial DSGE models that have one
system of equations, occasionally binding constraints with solutions demonstrating complementary slackness, regime switching models with systems corresponding to the status in each given regime or combinations of the above.

Consequently, will will have an aquation system an a gatekeeper logical expression indicating which equation system is in force for a given solution for a give $x_{t-1}, \epsilon_t$

\subsubsection{Worst Path: Pessimistic Error Bound}
\label{worst}

\subsubsection{Practical Considerations for Applying the Formula}
\label{sec:practicalformula}


\paragraph{MSNTO}
\paragraph{Assessing a Proposed Soluion}
\paragraph{RBC Example for Known Solution}


\begin{figure}
  \centering
\includegraphics{simpBoundsVActual.pdf}  
  \label{rbcTrunc}
  \caption{RBC Truncation Error Bound Versus Actual}
\end{figure}

\paragraph{RBC Example for Unknown Solution}
\paragraph{Algorithm Agnostic: Your Solution Here}

\section{An Algorithm for Improving Proposed Time Invariant Solutions}
\label{sec:algoforsoln}

\subsection{Function Approximation Representation}
\label{sec:funcApproxRep}

\subsubsection{General Issues}
\label{sec:generalissues}

\paragraph{Function composition}



\paragraph{Correct Expectations}

\paragraph{Precomputig Integrals}


\subsubsection{Smolyak Interpolation}
\label{sec:smolyakinterp}

\paragraph{Anisotropic}

\paragraph{Ergodic Set}

\subsection{Algorithm Overview}
\label{sec:algoverview}


\begin{figure}
  \centering
  


  \begin{gather}
    \fbox{Nonlinear Rational Expectations Model}\\ \Downarrow\\
\fbox{Bounded Time Invariant Function Solution}\\\Downarrow\\
\fbox{Series Representation}\\\Downarrow\\
\fbox{Series Approximation}
  \end{gather}
  \caption{From Models to Approximate Solutions}
  \label{fig:modelsto}
\end{figure}



This series representation for deterministic maps,
broadly applicable for nonlinear rational expectations models 
leads to a formulae for accuracy bounds for any proposed solution.
As an important component in an algorithm for
constructing approximate solutions it
facilitates exploiting recursive computation of the solutions for complicated models.
The algorithm has been implemented in
Mathematica code that can compute solutions for
nonlinear models with occasionally binding constraints and/or regime switching models.






\subsubsection{Generality}
\label{sec:generality}




\begin{itemize}
\item Time Invariant Maps
\item Bounded Solutions
\item Exists a Representation Problem is to Find One
\item Algorithm terminates with a Proposed Solution With Solution
  No further away from a true soltion than the specified tolerance 
\item No Existence Guarantees or Uniquess Guarantees.
\item Parallizable
\end{itemize}


Consider a family of {\it time invariant } stochastic functions:
 \begin{gather}
   \xWarg \in{R^L}\,\text{ with }\,\infNorm{\xWarg}  \le \bar{\mathcal{X}}\,\,\forall t> 0 \label{fFamily}.
 \end{gather}
The $x_{-1}$ is an  $L$ dimensional state vector and $\epsilon$ is a $K$ dimensional ``shock'' vector that together index
individual trajectories for future state vectors.  
Each member of the family characterizes the evolution of a {\em deterministic} trajectory of values.\footnote{Subsequent sections describe how these deterministic trajectories are useful for representing a wide array stochastic model solutions.}

It will prove useful to also define a time invariant deterministic function $\XtFuncTI\equiv \expctEps{\xtFuncTI}$ and denote
\begin{gather*}
\xsubtFunc{t+k}{(x_{t-1},\epsilon_t)}\equiv\begin{cases}
\xtFunc{(x_{t-1},\epsilon_t)} &k=0\\
\XtFunc{(\xsubtFunc{t+k-1}{(x_{t-1},\epsilon_t)})} &k>0
\end{cases}
\end{gather*}


Iterating conditional expectations of stochastic functions forward will lead
us to a series representation useful for representing dynamic model solutions.
Many rational expectations models have solutions characterized by a stochastic function $\mathcal{X}(x_{t-1},\epsilon_t):\mathcal{R}^n \times \mathcal{R}^k \rightarrow \mathcal{R}^n$ $\epsilon_t \sim iid$.
Consider Iterating the function $\mathcal{X}$ forward by 
recursively applying $\XtFunc{}$ to compute a solution path
\begin{gather}
\underbrace{(x_{t-1},\epsilon_t)} 
\underbrace{{\mathcal{X}}(x_{t-1},\epsilon_t)}
\underbrace{\xsubtFunc{t+1}{(x_{t-1},\epsilon_t)}}
\underbrace{\xsubtFunc{t+2}{(x_{t-1},\epsilon_t)}}
\underbrace{\ldots}
\intertext{Now, suppose this iteration produces bounded trajectories }
\infNorm{\xsubtFunc{t+k}{(x_{t-1},\epsilon_t)}}  \le \bar{\mathcal{X}}\,\,\forall s\ge 0 \label{fFamily}.
 \end{gather}
then it will then, be possible to write down a useful 
series representation for
the function $\mathcal{X}(x_{t-1},\epsilon_t)$.
These series will be based upon
discrepancies from an ``arbitrary'' Blanchard-Kahn linear dynamic
system.\footnote{As seen below, the linearity of the saddle point model
  will not preclude applying the technique to highly non linear models.}




  {Begin by Solving a Deterministic System at time $t$}
{\small

  \begin{itemize}
  \item For any given $\left (  \begin{bmatrix}
c_{t-1}\\k_{t-1}\\ \rcpC_{t-1}\\\theta_{t-1}
  \end{bmatrix}, \epsilon_t \right )=\tArg$ 
compute
  \begin{gather}
    \label{eq:3}
    x_t^1\tArg=B x_{t-1} + \phi \psi_e\epsilon_t + \phi z^1_t\tArg\\
    E_t(x^1_{t+1}\tArg)=B x^1_{t}\tArg
  \end{gather}
\item The model equations provide a deterministic system  for computing $  z^1_t=\begin{bmatrix}
    z^1_{1t}\tArg\\
    z^1_{2t}\tArg\\
    z^1_{3t}\tArg\\
    z^1_{4t}\tArg
  \end{bmatrix}$.
\item The solution satisfies the nonlinear model equations for one 
period and the stand-in linear model for subsequent periods.
\item Assess accuracy
  \end{itemize}
}




%   {Assess One Period Solution Accuracy}
%   \begin{itemize}
%   \item Show graphs of solution
%   \item Show graphs of error
%   \item Report accuracy assessment bounds
%   \end{itemize}



  {Nonlinear 2 Periods: Solve time $t$ Deterministic
    System }
{\small
  \begin{itemize}
  \item Compute $Z^1(x)= E_t(z^1_t(x,\epsilon_t))$
  \item For any given $\tArg$ 
compute
{\small
  \begin{gather}
    x_t^2\tArg=B x_{t-1} + \phi \psi_e\epsilon_t + \phi z^2_t\tArg + F \phi Z^1(x^2_t) \label{bothS}\\
    E_t(x^2_{t+1}\tArg)=B x^2_{t}\tArg+ \phi Z^1(x^2_t)
  \end{gather}
}
\item The model equations provide a deterministic system  for computing $  z^2_t=\begin{bmatrix}
    z^2_{1t}\tArg\\
    z^2_{2t}\tArg\\
    z^2_{3t}\tArg\\
    z^2_{4t}\tArg
  \end{bmatrix}$.
\item The solution satisfies the nonlinear model equations for two 
period and the stand-in linear model for subsequent periods.
\item unlike the first step, $x^2_t$ appears on both sides of equation \refeq{bothS}
\item  surprisingly, a simple fixed point iteration solves the nonlinear system
\item Assess accuracy
  \end{itemize}
}



\subsubsection{Approximating the Known Solution: $U(c) = Log(c)$ }
\label{sec:recov-known-solut}

\subsubsection{Approximating an Unknown Solution: $U(c) \ne Log(c)$ }
\label{sec:unknown-solutions}

\subsubsection{Occasionally Binding Constraints}
\label{sec:obc-solut}

The algorithm we have described,
uses a proposed deterministic map
characterizing the evolution of expected values for
the dynamic system going forward. It then solves
a deterministic problem at time t to improve the proposed solution.
There is nothing in the algorithm that precludes accommodating  inequality
constraints.\footnote{See section \ref{sec:regime-switch-model} characterizing
  models with regime switching.}

\begin{gather}
  h_i(x_{t-1},x_{t},x_{t+1},\epsilon_t)=h^{det}_{io}(x_{t-1},x_{t},\epsilon_t)+\sum_{j=1}^{p_i} [h^{det}_{ij}(x_{t-1},x_{t},\epsilon_t)h^{nondet}_{ij}(x_{t+1})]=0
\end{gather}

  {Assessing Rational Expectations Solution Accuracy}

The $m$ function codifies changes in model equations due to
inequality constraints or regime switching.$(\varpi \in \{1,\ldots,n\})$
we are  interested in finding a time invariant function $g^\ast$ that satisfies
 \begin{gather}
   \begin{split}
 h_{\varpi}(x_{t+s-1},g^\ast(x_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(x_{t+s-1},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) \label{theProblem} \\
\varpi= m(x_{t+s-1},g^\ast(x_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(x_{t+s-},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) 
   \end{split}\intertext{ for all $s>0$ where $\mathcal{H}$ is an operator,    that maps stochastic to deterministic functions}
  \end{gather}


For example, the Euler equations for the  neoclassical growth  model with
irreversible investment
\label{sec:simple-rbc-model-ext} can be written as
\begin{gather}
h_{10^{det}}(\cdot)=\frac{1}{c_t},\,\,
h_{11}^{det}()=\alpha \delta k_{t}^{\alpha-1} ,\,\,
h_{11}^{nondet}(\cdot)=E_t \left (\frac{\theta_{t+1}}{c_{t+1}} \right )\\
h_{20}^{det}(\cdot)=c_t + k_t-\theta_tk_{t-1}^\alpha,\,\,
h_{21}^{det}(\cdot)=0\\
h_{30}^{det}(\cdot)=\ln \theta_t -(\rho \ln \theta_{t-1} + \epsilon_t),\,\,
h_{31}^{det}(\cdot)=0\\
(k_t>0\land\theta_t>0\land c_t>0) \land
( (I_t>\gamma I^\ast \land \lambda_t>0)\lor (I_t=\gamma I^\ast \land \lambda_t\ge0))
\end{gather}
Since we will be working with models where expectations are computed at time t, $\epsilon_t$ is known.  Once again, the only stochastic components are those with time subscripts greater than t.


\paragraph{Proposed Conditional Expection}

\paragraph{Deterministic Problem}

\paragraph{Conditional Expectation Update}

\subsubsection{Algorithm Pseudocode}
\label{sec:pseudocode}

\subsubsection{RBC Example}
\label{sec:generalRBCExample}

\paragraph{unknown solutions}
\begin{description}
\item[Solution]
\item[Error Bound]
\end{description}

\paragraph{unknown solutions occasionally binding constraints}
\begin{description}
\item[Solution]
\item[Error Bound]
\item[resources]\
  \begin{itemize}
  \item 
  \end{itemize}
\end{description}


\subsection{ Other Examples}
\label{sec:otherexamples}

\subsection{regime switching}
\label{sec:regime}

\label{sec:regime-switch-model}

For example,
consider  the Barthelemy and Marx  Model 2: Regime Switching\cite{marxbarthelemy2012}


\cite{troy2007}
\begin{gather}
  \label{eq:4}
  i_t =E_t \pi_{t+1} + r_t\\
r_t= \rho r_{t-1} +u_t\\
i_t=\alpha_{s_t} \pi_t
\end{gather}

Bounded solutions if and only if all eigenvalues of 
\begin{gather}
  \label{eq:5}
  \begin{bmatrix}
    \frac{1}{|a_1|}&0\\
0&    \frac{1}{|a_2|}
  \end{bmatrix}
  \begin{bmatrix}
    p_{11}&p_{12}\\p_{21}&p_{22}
  \end{bmatrix}
\end{gather}
 are inside unit circle




  {Assessing Accuracy}
{\small

  \begin{itemize}
  \item As with series approximation,
 construct a family of bounded trajectories and compute
$  z_{t+s}(x_{t-1},\epsilon_t)$ as  %\footnote{These $z$ functions will soon prove useful in an algorithm for computing unknown trajectories like \refeq{fFamily}.}:
{
\begin{gather}
  z_{t+s}(x_{t-1},\epsilon_t) \equiv\\
   \begin{split}
 h_{\varpi}(\mathcal{X}_{t+s-1},g^\ast(\mathcal{X}_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(\mathcal{X}_{t+s-1},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) \label{theProblem} \\
\varpi= m(\mathcal{X}_{t+s-1},g^\ast(\mathcal{X}_{t+s-1},\epsilon_{t+s}),\mathcal{H}[g^\ast(g^\ast(\mathcal{X}_{t+s-},\epsilon_{t+s}),\epsilon_{t+s+1})],\epsilon_{t+s}) 
   \end{split}
  \end{gather}
}
\item The formula \refeq{theSeries} provides information about how much $x_{t}$ would need
to change in order for the trajectory to honor the constraints along the path
\item An exact solution should produce zero for all the $z$ functions
\item One can use a truncated series to carry out the approximation for changes in $x_t$
\item Useful to have measures of accuracy that don't rely upon knowing the solution beforehand
\item can adopt Judd approach for loss function for characterizing importance of errors
  \end{itemize}
}


  {Barthelemy and Marx  Model 2: Regime Switching
\cite{marxbarthelemy2012}}


\cite{troy2007}
\begin{gather}
  \label{eq:4}
  i_t =E_t \pi_{t+1} + r_t\\
r_t= \rho r_{t-1} +u_t\\
i_t=\alpha_{s_t} \pi_t
\end{gather}

Bounded solutions if and only if all eigenvalues of 
\begin{gather}
  \label{eq:5}
  \begin{bmatrix}
    \frac{1}{|a_1|}&0\\
0&    \frac{1}{|a_2|}
  \end{bmatrix}
  \begin{bmatrix}
    p_{11}&p_{12}\\p_{21}&p_{22}
  \end{bmatrix}
\end{gather}
 are inside unit circle





\section{Future Work}
\label{sec:future}

\begin{itemize}
\item time varying:  regime switching, hammers need nails
\item SVMR and other ML universal approximators



Support vector machines occupy a prominent place applied machine learning
Additionally they can reduce the burden of computation as they determine 
a subset of points that are important in characterizing a given function.
Kernel trick,  powerful representation quadratic programming solution to compute there are on-line techniques for adding and deleting individual points from the representation.  A weighted sum of kernel functions RBF, wavelet, special forms for time series.
  This paper proposes a new series representation for bounded so-
lutions to dynamic models. This series representation can be used
to determine a series representation for time invariant discrete time
maps that characterize the solutions to many models. Consequently,
the technique constitutes an important component in a technique for
accurately characterizing the solutions for a wide array of nonlinear
rational expectations models. It can also provide a formula for com-
puting accuracy bounds for any proposed time invariant model so-
lution. The series representation serves as an important component
in an algorithm for constructing approximate solutions for nonlinear
rational expectations models.
The technique recursively computes solutions that honor the con-
straints for successively longer horizons.  The solutions computed by
the technique accommodate the possibility that model trajectories can
depart from and re-engage inequality constraints as well as transition
between various regimes.
This paper applies support vector machine function approximation to
reduce the computational burden associated with representing the
unknown, potentially highly nonlinear stochstic functions that arise in
solving dynamic models with occasionally binding constraints.


Support vector machines 
have become an essential tool in contemporary machine learning research
where computer scientists exploit their flexibility and
computational tractability in modelling complex high dimensional data.
Like many other function approximation approaches,
support vector machines represent functions as a linearly weighted sum
of a family of basis functions.  They differ from other approaches in  the
use of ``hinge loss functions'' that generate
an easy to solve
quadratic programming problem(QPP) for determining the weights.
The solution of this QPP identifies a subset of points, the support vectors,
that are influential in the representation.  With strategically chosen
basis functions, this can dramatically reduce the number of terms needed
to approximate a function to a given level of accuracy.




\item function composition Fa Di Bruno
\item divide and conquer solution space initially collect results stocastic steady state at center of divided region with overlap
\item dynamical system theory implications
\item relation to Koopman Operators
\item Implications of almost arbitrary.  Small F eigenvalues versus small z values
\item implement other languages
\end{itemize}

\section{Conclusions}
\label{sec:conc}




\bibliographystyle{plainnat}
\bibliography{anderson,files}

\end{document}

