\documentclass[12pt]{article}


% \usepackage[authoryear]{natbib}
% \usepackage{amsmath}
% \usepackage{hyperref}
% \usepackage{hyperref}
% \usepackage{geometry}
% \usepackage{graphicx}
% \usepackage{amsfonts}

\input{../../paperProduction/occBind/docs/AMArepresentationNewCmds}



\author{Gary S. Anderson\thanks{The analysis and conclusions set forth are those of the author and do not indicate concurrence by other members of the research staff or the Board of Governors. I would like to thank Luca Guerrieri, Christopher Gust, Hess Chung, Benjamin Johannsen  and Robert Tetlow for their comments and suggestions.  Special thanks to Luca Guerrieri for first noticing that the series representation formulation could lead to an error bound for model solutions.}}

\title{A New Series Representation for 
Nonlinear Dynamic Stochastic Model Solutions: General Error Bounds and a New Solution Algorithm}

\date{\today: \currenttime}
\begin{document}

\maketitle

\begin{abstract}
This paper proposes a new series representation for bounded time series paths and uses this representation to construct error bounds for proposed solutions to a very general class of dynamic stochastic models including models subject to occasionally binding constraints and/or regime switching.
This error bound calculation which, incorporating the impact of errors across  all the equations in 
the system makes, it possible to compute, ahead of time, estimates of the computational cost for a
given level of accuracy of each component of the solution.
These error bounds should prove useful for assessing the accuracy of any
proposed model solution regardless of the source.


In addition, the paper also proposes a new algorithm for solving these models.
The series representation makes it possible to reliably improve upon an initial
guess for a stochastic model solution by solving a 
potentially complicated, but deterministic problem in the initial time period.
Like the error bound formula, the algorithm is appropriate for models with occasionally binding constraints and/or regime switching. 


The  paper uses a particular implementation of the algorithm to
demonstrate how to use the 
series representation in conjunction with 
Smolyak polynomial function approximation to avoid repeated numerical integration
and to exploit the high degree of parallelism available in the algorithm.







\end{abstract}

\newpage
\tableofcontents
\newpage

\section{Introduction}





Stochastic dynamic non linear economic
models increasingly embody  occasionally binding constraints (OBC).
Since \cite{Christiano2000} a host of
authors have described a variety of approaches. 
\cite{holden15:_exist_dsge,guerrieri15:_occbin,benigno09,hintermaier10,brumm10,nakov08,haefke98,nakata12,gordon11,billi11,Hintermaier2010,Guerrieri2015}
This paper provides yet another.  This new series representation provides  a coherent framework for attacking a wide variety of complicated nonlinear models.

The analysis develops a new series representation for bounded time series.  The author has found no comparable use of a reference dynamical system to generate a transformation of one infinite dimensional series into another. 

The framework provides a new way to bound the error one can expect from
employing a given proposed model solution and leads to a
algorithm with  components similar to parameterized expectations that
one can use to improve proposed solutions. The series representation makes
it possible to organize the calculation around computing a deterministic
problem at time t given a proposed solution.  The deterministic solution
can accommodate inequality constraints or alternative regimes to produce a
solution for each set of initial conditions.  One can typically arrange,
perhaps by adding auxiliary variables, to produce a ``decision rule''
that one can use to correctly precompute a deterministic conditional
expectation function that can be iterated forward and serves to
improve upon the original proposed solution.
Time invariant stochastic functions 
lead naturally to an associated family of deterministic maps
which can conveniently represented by the series representation.


Section \ref{sec:newseries} presents a useful new series representation for any bounded time series.
Section \ref{sec:extToMaps} shows how to apply this series representation to time invariant maps.
Section \ref{sec:solnerrorbounds} provides formulas for computing dynamic stochastic model error bounds for proposed  solutions.
Section \ref{sec:algoforsoln} presents a new solution algorithm.
Section \ref{sec:future} discusses directions for future work.
Section \ref{sec:conc} concludes.

\section{A New Series Representation For  Bounded Time Series}
\label{sec:newseries}

\subsection{Linear Reference Models and a Formula for  ``Anticipated Shocks''}
\label{sec:linref}




For any linear homogeneous 
$L$ dimensional 
deterministic 
system 
\begin{gather}
  	 H_{-1} x_{t-1} + H_0 x_t + H_1 x_{t+1}=0\label{hSystem}
\end{gather}
that produces  a unique stable solution, 
it is well known\ \citep{anderson10} that  inhomogeneous solutions 
\begin{gather}
	 H_{-1} x_{t-1} + H_0 x_t + H_1 x_{t+1}=\psi_\epsilon \epsilon +\psi_{c}
\intertext{ can be computed as}
x_t=B x_{t-1} + \phi \psi_\epsilon \epsilon + (I - F)^{-1} \phi \psi_c
\intertext{where}
\phi= (H_0 +H_1 B)^{-1}  \text{ and } \,\,F=-\phi H_1 
\end{gather}
It will be useful to collect the components of this representation for use in
subsequent sections of the paper.
Define $\linMod \equiv \linModMats$.




\begin{theorem}
Consider an arbitrary, but bounded path
 \begin{gather}
   \xWOarg_t \in{R^L}\,\text{ with }\,\infNorm{\xWOarg_t}  \le \bar{\mathcal{X}}\,\,\,\,\,\forall t> 0 \label{aBPath}.
 \end{gather}

Now, given the trajectories \refeq{aBPath}, define 
$  z_{t}$ as  
\begin{gather}
  z_{t} \equiv H_{-1} \xWOarg_{t-1} +  H_0 \xWOarg_{t} +  H_1 \xWOarg_{t+1} \label{defZ} 
\end{gather}

One can then express the $\xWOarg_t$ as 

	 \begin{gather}
	 \xWOarg_{t} =B x_{t-1}+ \phi \psi_\epsilon\epsilon + (I - F)^{-1} \phi \psi_c + \sum_{\sForSum=0}^\infty F^s \phi z_{t+\sForSum} \label{theSeries}
\intertext{and}
	 \xWOarg_{t+k+1} =B \xWOarg_{t+k}  + (I - F)^{-1} \phi \psi_c+ \sum_{\sForSum =0}^\infty F^\sForSum \phi z_{t+k+\sForSum+1} \,\,\,\,\,\forall t,k \ge  0.
	 \end{gather}
\end{theorem}

\begin{myProof}
See \citep{anderson10},
\end{myProof}

	 Consequently, given a bounded time series \refeq{aBPath},
and a stable linear homogeneous system like \refeq{hSystem},
one can easily compute a corresponding series representation 
\refeq{theSeries}.
Interestingly, the linear model, $H$, the  constant term $\psi_c$ and the
impact of the stochastic shocks $\psi_\epsilon $ can  be 
chosen rather arbitrarily -- the only constraint being the existence of a saddle-point solution for the linear system.  The formula will provide a series  for any $L$ dimensional $\linMod$.



Under certain rank conditions, this transformation is one to one and onto.  The
transformation $ {\{ x_{t}, x_{t}, x_{t+1},x_{t+2}\ldots\}} \rightarrow \linMod(x_{t-1}) \rightarrow{\{ z_{t}, z_{t+1}, z_{t+2},\ldots\}} $ is invertible under
certain rank conditions. $ {\{ z_{t}, z_{t+1}, z_{t+2},\ldots\}} \rightarrow \linMod(x_{t-1})^{-1} \rightarrow{\{ x_{t}, x_{t}, x_{t+1},x_{t+2}\ldots\}} $ and the inverse can be interpreted as giving the impact of ``fully anticipated future shocks'' on the path of $x_t$ obeying the solution of a linear rational expectations model.  The formula supports the intuition that far distant shocks influence current conditions less than  imminent shocks.

A key feature to note is that the series representation can accommodate arbitrarily complex time series trajectories, so long as these trajectories are bounded.
Later,this observation will give us some confidence in the 
robustness of the algorithms described in section 
\ref{sec:unknown-solutions} for constructing series 
representations for unknown families of functions 
satisfying complicated systems of dynamic non-linear equations.

\subsection{An Example: An ``Almost'' Arbitrary Linear Model and  Some Arbitrary  Bounded Time Series}
\label{sec:almostarbitrary}


Consider the following constructed from ``almost'' arbitrary coefficients
\begin{gather}
  \begin{bmatrix}
H_{-1}&H_{0}&H_{1} 
  \end{bmatrix}=
\vcenter{\hbox{\includegraphics{refHmat.pdf}}}\intertext{with $\psi_c=\psi_\epsilon=0, \,\,  \psi_z=I$.
These coefficients are not completely arbitrary in so far as the series 
representation requires that the linear model
has a unique stable solution.}
  B=
\vcenter{\hbox{\includegraphics{refBmat.pdf}}}\\
\phi=
\vcenter{\hbox{\includegraphics{refPhimat.pdf}}}\\
F=
\vcenter{\hbox{\includegraphics{refFmat.pdf}}}
\end{gather} 



The existence of a series 
representation requires only that the state values along the 
paths remain bounded.  For example, consider the following three
bounded families of time series paths:
\begin{gather}
  x_{1,t}=\alpha D_\pi(t) \\
x_{2,t}=\beta (-1)^t\\
x_{3,t}=\epsilon_t 
\end{gather} 
where $D_\pi(t)$ gives the t-th digit of $\pi$ and the $\epsilon_t$ are a sequence of pseudo random draws from the uniform distributions $\mathcal{U}(-4,4)$

The first set of trajectories(See Figure \ref{arbpaths}) is a function of
the digits in the decimal representation of $\pi$.  
The second set of trajectories (See Figure \ref{arbpaths}) oscillates between two values
determined by  a nonlinear function of the initial conditions and the shock.
The third set of trajectories (See Figure \ref{arbpaths}) is a sequence of uniformly distributed random
numbers based on a seed determined by  a nonlinear function of  the initial conditions and the shock.
These examples paths were chosen to emphasize that the trajectories
 need not converge to a fixed point, and 
need not be produced by iteration of a discrete-time map.
The boundedness of the paths is a sufficient condition for the existence 
of the series representation.\footnote{Although useful in some contexts,
this paper will not investigate representations for families of
unbounded trajectories.}


\begin{figure}
  \centering
\includegraphics[width=2in]{piPath.pdf}
\includegraphics[width=2in]{oscillPath.pdf}
\includegraphics[width=2in]{pseudoPath.pdf}
\includegraphics[width=3in]{theZs.pdf}  
  
  \caption{Arbitrary Bounded Time Series Paths and Corresponding $z_{i,t}$ values}\label{arbpaths}
\end{figure}


% \begin{figure}
%   \centering
% \includegraphics[width=2in]{oscillPath.pdf}
  
%   \caption{Oscillatory Path}\label{oscillpath}
% \end{figure}

% \begin{figure}
%   \centering
% \includegraphics[width=2in]{pseudoPath.pdf}
%   \caption{Uniformly Distributed Path}\label{pseudopath}
% \end{figure}

% \begin{figure}
%   \centering
% \includegraphics[width=3in]{theZs.pdf}  
%   \caption{The  z's Corresponding to  $x_{-1}=(1,2,3),\epsilon=(2,1,2)$} \label{arbFig}
% \end{figure}



Figure \ref{arbpaths} shows, for a particular initial state vector and shock value,  the paths for the state vectors and the  z's that generate the path.
One can repeat the calculations for any given initial condition to produce
a z series exactly replicating the given set of trajectories.  The family
of z functions along with equation \ref{theSeries} provide a series 
representation for the family of trajectories. 





\subsection{A Path Norm and Assessing $x_t$ Error}
\label{sec:truncationerr}
The formula \ref{theSeries} was derived to compute he impact on the current state of fully anticipated future shocks.  The formula characterizes the impact exactly.  However one can contemplate the impact of at least two types of imprecision.  One could truncate a series of correct values of $z_t$.  One might have imprecise values of $z_t$ along the path.

\subsubsection{Truncation Error}


One could consider approximating $\mathcal{X}_t$ by 
truncating the series \ref{theSeries} at a finite number of terms.
 	 \begin{gather}
 	 \xWOargK_t \equiv B x_{t-1}+ \phi \psi_\epsilon\epsilon  + (I - F)^{-1} \phi \psi_c + \sum_{s=0}^k F^s \phi z_{t}\label{theTruncSeries}
 \end{gather}
We can bound the  series approximation truncation errors.
Since
    \begin{gather}
      \label{eq:1}
\sum_{s=k+1}^{\infty} F^s \phi \psi_z = (I -F)^{-1} F^{k+1}\phi \psi_z       \\
\infNorm{\xWarg-\xWargK} \le \infNorm{(I -F)^{-1} F^{k+1}\phi \psi_z} \left ( \infNorm{H_{-1} }+ \infNorm{H_{0} }+ \infNorm{H_{1} } \right )\bar{\mathcal{X}}
    \end{gather}
Note that for approximating $\xWarg$ the impact of  a given realization along the path declines for those realizations which are  more temporally distant.

 Figure \ref{figArbTrunc} shows
that the truncation error bound is a very conservative measure of the accuracy
of the truncated series.  The series requires only the first 20 terms to compute
the initial value of the state vector to machine precision. 
The series representation can compute the entire series to machine precision
if all the terms are included, but the terms for state vectors closer 
to the initial time have the most important impact.


\begin{figure}
  \centering


\includegraphics[width=3in]{arbTruncErr.pdf}  
  \caption{$x_t$ Error Bound Versus Actual Error} \label{figArbTrunc}

\end{figure}

\subsubsection{Path Error}


We can assess the impact of incorrect values for $z_t$ by computing the maximum correction required for the $z_t$ and applying the 
same formula.
One could consider approximating $\mathcal{X}_t$ using
 	 \begin{gather}
 	 \xWOargK_t \equiv B x_{t-1}+ \phi \psi_\epsilon\epsilon  + (I - F)^{-1} \phi \psi_c + \sum_{s=0}^\infty F^s \phi (z_{t}+\Delta z_{t})\label{theDeltaSeries}
 \end{gather}
Note that for approximating $\xWarg$ the impact of  a given realization along the path declines for those realizations which are  more temporally distant.
However, we can bound the  series approximation  errors.
Since
    \begin{gather}
\infNorm{\xWarg-\xWargK} \le \infNorm{(I -F)^{-1} \phi \psi_z}  \infNorm{\Delta z_t } \label{pathErr}
    \end{gather}


\label{sec:pathnorm}

\section{Nonlinear Dynamic Stochastic Time Invariant Maps}
\label{sec:extToMaps}



\subsection{Application to Time Invariant Maps}


In this paper we will be interested in computing with time
 invariant maps. Many dynamic stochastic models have solutions that 
fall in this class. 
These time invariant maps generate  bounded time series paths which can
be represented using the framework from section \ref{sec:newseries}.
These time invariant maps impose additional structure on the time 
series they generate.  This structure will allow us to use the series formula to
develop bounds for the error in the solution.
In this section, to keep things simple, we will rule out regime switching and occasionally binding constraints and start
 with  a simple single equation system?
Later, in order to handle models with regime switching and occasionally binding constraints, will need to consider more complicated collections of 
of equation systems with  Boolean gates. We will show how to apply the 
series formulation to get error bounds for these models as well.



\subsubsection{An RBC Example}
\label{sec:rbcaux}
  See \cite{Maliar2005}
 \begin{gather*}
   \max\left \{  u(c_t^t) + E_t \sum_{\tau=t}^\infty \beta \delta^{\tau+1-t}u(c_{\tau+1}^t)\right \}\\
c_\tau^t + k_\tau^{t+1}=(1-d)k_\tau^{t-1} + \theta_\tau f(k_\tau^{t-1})\\
f(k_\tau^{t-1})= k_\tau^\alpha
 \end{gather*}
The well known first order conditions for the model are

\begin{gather}
\frac{1}{c_t^{\eta}}=\alpha \delta k_{t}^{\alpha-1} E_t \left (\frac{\theta_{t}}{c_{t+1}^\eta} \right ) \\
c_t + k_t=\theta_{t-1}k_{t-1}^\alpha \\
 \theta_t =\theta_{t-1}^\rho e^{\epsilon_t}\label{rbcSys}
 \end{gather}


\paragraph{An RBC Model Example Known Solution ($\eta=d=1$)}
\label{sec:rbcexample}
  {The RBC Model ($\eta=d=1$): Recovering a Known Solution}


 \begin{gather}
\frac{1}{c_t}=\alpha \delta k_{t}^{\alpha-1} E_t \left (\frac{\theta_{t}}{c_{t+1}} \right ) \\
c_t + k_t=\theta_{t-1}k_{t-1}^\alpha \\
\theta_t =\theta_{t-1}^\rho e^{\epsilon_t}\label{rbcSys}
\intertext{and there is a closed form solution}
c_t=  (1-\alpha \delta) \theta_{t} k_{t-1}^\alpha\\
  k_{t}= \alpha \delta \theta_{t} k_{t-1}^\alpha.\label{soln}\\
\theta_t =\theta_{t-1}^\rho e^{\epsilon_t}
\end{gather}
For mean zero iid $\epsilon_t$ we can easily compute a family of bounded trajectories




\begin{gather}
  \begin{bmatrix}
c_{t+s}(k_{t-1},\theta_t,\epsilon_t)\\k_{t+s}(k_{t-1},\theta_t,\epsilon_t)    \\ \theta_{t+s}(\theta_{t-1},\theta_t,\epsilon_t)    
  \end{bmatrix}
\intertext{with conditional mean converging over time to }
  \begin{bmatrix}
    c_{ss}\\k_{ss}
  \end{bmatrix}=
  \begin{bmatrix}
\nu^\alpha-\nu\\ \nu
  \end{bmatrix}\intertext{where}
\nu= \alpha ^{\frac{1}{1-\alpha }} \delta ^{\frac{1}{1-\alpha }}
\end{gather}

We can also compute the conditional expectation of the model variables for any given $\theta_{t-1},k_{t-1}$
\begin{gather*}
  E_t(c_t|\theta_{t-1},k_{t-1})=(1-\alpha\delta)k_{t-1}^\alpha e^{\frac{\sigma^2}{2}}\theta_{t-1}^\rho\\
  E_t(k_t|\theta_{t-1},k_{t-1})=\alpha\delta k_{t-1}^\alpha e^{\frac{\sigma^2}{2}}\theta_{t-1}^\rho\\
  E_t(\theta_t|\theta_{t-1},k_{t-1})=e^{\frac{\sigma^2}{2}}\theta_{t-1}^\rho
\end{gather*}
Consequently, we can use the arbitrary linear system along with the decision rule to compute values for $c_t$ and $k_t$ for any intial values for $k_{t-1}$ and $\theta_{t-1}$ and realization of $\epsilon_t$.
By applying the law of iterated expectations, we can compute conditional expected solution paths forward from any initial values 
and realization of $\epsilon_t$.
As a result, we can use the family of conditional expectations
along with the contrived reference model to recover an 
approximation for equation \refeq{soln} along with error bounds.
The series representation provides a weighted sum of z functions that give us
an approximation for the known solution.
Note that the reference model is deterministic and the $z$ functions account for the stochastic nature of the model.

For any given values of $k_{t-1},\theta_{t-1}, \epsilon_t$ the model solution and conditional expectations path produces paths for $z_{1t}, z_{2t}, z_{3t}$

\begin{gather*}
  \begin{bmatrix}
c_t(k_{t-1},\theta_{t-1}, \epsilon_t)\\
k_t(k_{t-1},\theta_{t-1}, \epsilon_t)\\
\theta_t(k_{t-1},\theta_{t-1}, \epsilon_t)
  \end{bmatrix} \rightarrow
  \begin{bmatrix}
  z_{1t}(k_{t-1},\theta_{t-1}, \epsilon_t)\\
  z_{2t}(k_{t-1},\theta_{t-1}, \epsilon_t)\\
  z_{3t}(k_{t-1},\theta_{t-1}, \epsilon_t) 
  \end{bmatrix}\equiv z(k_{t-1},\theta_{t-1}, \epsilon_t)\intertext{where}
  \begin{bmatrix}
c_t(k_{t-1},\theta_{t-1}, \epsilon_t)\\
k_t(k_{t-1},\theta_{t-1}, \epsilon_t)\\
\theta_t(k_{t-1},\theta_{t-1}, \epsilon_t)
  \end{bmatrix}  =
B   \begin{bmatrix}
c_{t-1}\\
k_{t-1}\\
\theta_{t-1}
  \end{bmatrix}  + \phi \psi_\epsilon\epsilon_t + (I - F)^{-1} \phi \psi_c + \sum_{\sForSum=0}^\infty F^s \phi z_{t+\sForSum}(k_{t-1},\theta_{t-1}, \epsilon_t) 
\end{gather*}
% \footnote{
% We need not  make these adjustments for the steady state,
% but doing so economizes on the number of terms 
% required for a given level of approximation
% accuracy.}

For example, using the following parameter values and using the arbitrary linear reference model we can generate a series representation for the model solutions.

\begin{gather}
\vcenter{\hbox{\includegraphics{../../paperProduction/occBind/docs/RBCParamSubs.pdf}}} \,\, \text{ we have } \,\,
  \begin{bmatrix}
    c_{ss}\\k_{ss} \\ \theta_{ss} \label{rbcparams}
  \end{bmatrix}=
\left [ \vcenter{\hbox{\includegraphics{RBCSSVal.pdf}}}\right ]
\end{gather}

With 
\begin{gather*}
  \begin{bmatrix}
 k_{t-1}\\\theta_{t-1}\\\epsilon_t
  \end{bmatrix}=
\left [ \vcenter{\hbox{\includegraphics{anXEps.pdf}}}\right ]
\end{gather*}


\begin{figure}
  \centering
\includegraphics[width=2.5in]{simprbcvals.pdf}  
\includegraphics[width=2.5in]{simprbczvals.pdf}  
  \label{rbcpaths}
  \caption{model variable values and z values}
\end{figure}

The figure   \ref{rbcTrunc} shows the impact that truncating the series has 
on the initial value of the state variables.   The bound again is very pessimistic. Even with an arbitrarily chosen linear model, ultimately the series approximation provides an accurate value for the initial state vector.  Using a linearization that better tracks the nonlinear model paths, improves the approximation.

\begin{figure}
  \centering
\includegraphics{simpArbBoundsVActual.pdf}  
  \label{rbcTrunc}
  \caption{RBC Truncation Error Bound Versus Actual}
\end{figure}


Using the linearization of the RBC model produces a tighter but still pessimistic bound on the errors for the initial state vector.
The first few terms make most of the difference in approximating the value of the state variables.


\begin{figure}
  \centering
\includegraphics{simpBoundsVActual.pdf}  
  \label{rbcTrunc}
  \caption{RBC Truncation Error Bound Versus Actual}
\end{figure}

For convenience of notation in what follows, 
we will focus on models built up from components of the form
\begin{gather}
  h_i(x_{t-1},x_{t},x_{t+1},\epsilon_t)=h^{det}_{io}(x_{t-1},x_{t},\epsilon_t)+\sum_{j=1}^{p_i} [h^{det}_{ij}(x_{t-1},x_{t},\epsilon_t)h^{nondet}_{ij}(x_{t+1})]=0
\end{gather}
This is a very broad class of models including most widely used
macroeconomics models.

For example, the Euler equations for the  neoclassical growth  model 
\label{sec:simple-rbc-model-ext} can be written as
\begin{gather}
h_{10^{det}}(\cdot)=\frac{1}{c_t},\,\,
h_{11}^{det}()=\alpha \delta k_{t}^{\alpha-1} ,\,\,
h_{11}^{nondet}(\cdot)=E_t \left (\frac{\theta_{t+1}}{c_{t+1}} \right )\\
h_{20}^{det}(\cdot)=c_t + k_t-\theta_tk_{t-1}^\alpha,\,\,
h_{21}^{det}(\cdot)=0\\
h_{30}^{det}(\cdot)=\ln \theta_t -(\rho \ln \theta_{t-1} + \epsilon_t),\,\,
h_{31}^{det}(\cdot)=0
\end{gather}
Since we will need to compute 
the conditional expectation of nonlinear expressions,  
this setup will make it possible for us to use auxiliary
variables to correctly compute the required expected values.

It is worth noting that since we will be working with models where expectations are computed at time t, with  $\epsilon_t$  known,  the only stochastic components are those with time subscripts greater than $t$. 





We construct our linear reference model, $\linMod$, by augmenting the RBC model with the equation
\begin{gather}
  \rcpC_t=\frac{1}{c_t}
\end{gather}
substituting $\rcpC_{t+1}$ for $\frac{1}{c_{t+1}}$ in the first equation and 
 linearizing the RBC model about the ergodic mean
given in \refeq{rbcparams}
{\small
\begin{gather}
  \begin{bmatrix}
H_{-1}&H_{0}&H_{1} 
  \end{bmatrix}=\\
\vcenter{\hbox{\includegraphics{RBCHmatSymb.pdf}}} \label{rbcLinSys}
\intertext{with}
\psi_\epsilon=
\begin{bmatrix}
  0\\0\\1\\0
\end{bmatrix}, \psi_z=I
\end{gather}%(\footnote{generated by AMAPaperCalcs.mth {RBCHmatSymb.pdf}})
}
These coefficients  produce a unique stable linear solution.

\begin{gather}
  B=
\vcenter{\hbox{\includegraphics{RBCBmatSymb.pdf}}},
\phi=
\vcenter{\hbox{\includegraphics{RBCPhimatSymb.pdf}}}\\
F=
\vcenter{\hbox{\includegraphics{RBCFmatSymb.pdf}}}\\
\psi_c=
\vcenter{\hbox{\includegraphics{RBCHSum.pdf}}}
\vcenter{\hbox{\includegraphics{RBCSS.pdf}}}=\vcenter{\hbox{\includegraphics{RBCPsissSymb.pdf}}}
\end{gather}
\section{Computing Model Solution Error Bounds}
\label{sec:solnerrorbounds}

We will consider time invariant maps that arise form solving a time t optimization problem codified in a collection of systems of equations.  We will assume that we have a countable collection of equations systems that are mutually exclusive and exhaustive.  Given $\epsilon_t, x_{t-1}$,  this collection of 
systems of equations produces a unique solution for $x_t$.  We will include 
enough auxiliary variables so that we can correctly compute expected values
for model variables by applying the law of iterated 
expectations to individual variables.
The $x_t$ satisfies one and only one of the 
collection of equations and$x_t$ is (locally) the unique solution 
for the collection of
equations.  Examples of such systems include typical DSGE models that have one
system of equations, occasionally binding constraints with solutions demonstrating complementary slackness, regime switching models with systems corresponding to the status in each given regime or combinations of the above.

It will be convenient for describing the algorithms to
 augment the model equations to include auxiliary variables so that we can write the model in the form
$ \eqnFuncSys$.  Where, each 
\begin{gather}
\eqnFuncSysI{i}\equiv \eqnFuncSysIExpl{i}
\end{gather}
 represents a set of model equations, $\eqnFunc_i$,  and boolean valued gate, $\gate_i$. Consequently, will will have an equation system and a gatekeeper logical expression indicating which equation system is in force for a given solution for a give $x_{t-1}, \epsilon_t$  They represent exhaustive and mutually exclusive equation systems determining the solution at time t for the model.  They could be complementary slackness conditions or could represent a model with different regimes. We will consider time invariant model solutions $x_t=\xWarg$.  Given $\xWarg$, $\XWOarg(x_{t-1})\equiv \expct{\xWarg}$. We will have



 Given an exact solution $x^\star_t=g^\star(x_{t-1},\epsilon_t)$ define
  \begin{gather}
G^\star(x)\equiv\expct{g^\star(x,\epsilon)} \intertext{then with}
E_tx^\star_{t+1}=G^\star(g^\star(x_{t-1},\epsilon_t))\\
    \label{eq:2}
\eqnFunc(x^\star_{t-1},x^\star_t,E_tx^\star_{t+1},\epsilon_t)=0  \,\, \forall  (x_{t-1},\epsilon_t)\\ \intertext{Using $G^\star$ and $\linMod$ construct the family of trajectories and corresponding $z^\star_t(x_{t-1},\epsilon)$ }
   x^\star_t(x_{t-1},\epsilon_t) \in{R^L}\,\,\infNorm{x^\star_t(x_{t-1},\epsilon_t)}  \le \bar{\mathcal{X}}\,\,\forall t\ > 0
  \end{gather}
   \begin{align}
   z^\star_{t}(x_{t-1},\epsilon_t) \equiv& H_{-1}  x^\star_{t-1}(x_{t-1},\epsilon_t) + \nonumber\\
 & H_0  x^\star_{t}(x_{t-1},\epsilon_t) +  \label{defZ} \\
 & H_1  x^\star_{t+1}(x_{t-1},\epsilon_t). \nonumber
   \end{align}




   The exact solution has a representation given by
	 \begin{gather}
	 x^\star_{t}(x_{t-1},\epsilon) =B x_{t-1}+ \phi \psi_\epsilon\epsilon + (I - F)^{-1} \phi \psi_c +\\ \sum_{\sForSum=0}^\infty F^s \phi z^\star_{t+\sForSum}(x_{t-1},\epsilon) \intertext{and}
	 \expct{x^\star_{t+1}(x_{t-1},\epsilon)} =B x^\star_{t+k} + \sum_{\sForSum =0}^\infty F^\sForSum \phi \expct{z^\star_{t+1+\sForSum}(x_{t-1},\epsilon)} + (I - F)^{-1} \phi \psi_c 
 \intertext{with}
 \eqnFunc(x_{t-1},x^\star_t,E_tx^\star_{t+1},\epsilon_t)=0  \,\, \forall  (x_{t-1},\epsilon_t)\\ 
	 \end{gather}

Now consider a proposed solution for the model,
 $x^p_t=g^p(x_{t-1},\epsilon_t)$ define
$G^p(x)\equiv\expct{g^p(x,\epsilon)}$  so that 
  \begin{gather}
E_tx_{t+1}=G^p(g^p(x_{t-1},\epsilon_t))\\
\mathbf{e}_t(x_{t-1},\epsilon)\equiv
\eqnFunc(x_{t-1},x^p_t,E_tx^p_{t+1},\epsilon_t)\\\intertext{Using $G^p$ and $\linMod$ construct the family of trajectories and corresponding $z^p_t(x_{t-1},\epsilon)$ }
   x^p_t(x_{t-1},\epsilon_t) \in{R^L}\,\,\infNorm{x^p_t(x_{t-1},\epsilon_t)}  \le \bar{\mathcal{X}}\,\,\forall t\ > 0
  \end{gather}
   \begin{align}
   z^p_{t}(x_{t-1},\epsilon_t) \equiv& H_{-1}  x^p_{t-1}(x_{t-1},\epsilon_t) + \nonumber\\
 & H_0  x^p_{t}(x_{t-1},\epsilon_t) +  \label{defZ} \\
 & H_1  x^p_{t+1}(x_{t-1},\epsilon_t). \nonumber
   \end{align}








 The proposed solution has a representation given by 
  \begin{gather}
    \label{eq:4}
	 x^p_{t}(x_{t-1},\epsilon) =B x_{t-1}+ \phi \psi_\epsilon\epsilon + (I - F)^{-1} \phi \psi_c +\\ \sum_{\sForSum=0}^\infty F^s \phi z^p_{t+\sForSum}(x_{t-1},\epsilon) 
 \intertext{and}
 	 \expct{x^p_{t+1}(x_{t-1},\epsilon)} =B x^p_{t+k} + \sum_{\sForSum =0}^\infty F^\sForSum \phi z^p_{t+1+\sForSum}(x_{t-1},\epsilon) + (I - F)^{-1} \phi \psi_c \intertext{with}
\mathbf{e}_t(x_{t-1},\epsilon)\equiv
\eqnFunc(x_{t-1},x^p_t,E_tx^p_{t+1},\epsilon_t)
  \end{gather}




  \begin{gather}
    \label{eq:3}
	 x^\star_{t}(x_{t-1},\epsilon) -	 x^p_{t}(x_{t-1},\epsilon) =
\sum_{\sForSum=0}^\infty F^s \phi (z^\star_{t+\sForSum}(x_{t-1},\epsilon)-z^p_{t+\sForSum}(x_{t-1},\epsilon))     \\
	 x^\star_{t}(x_{t-1},\epsilon) -	 x^p_{t}(x_{t-1},\epsilon) =
\sum_{\sForSum=0}^\infty F^s \phi \Delta z_{t+\sForSum}(x_{t-1},\epsilon_t)   \\ 
	\infNorm{ x^\star_{t}(x_{t-1},\epsilon) -	 x^p_{t}(x_{t-1},\epsilon)} \le
\sum_{\sForSum=0}^\infty F^s \phi \infNorm{\Delta z_{t+\sForSum}(x_{t-1},\epsilon_t)}    
  \end{gather}

By bounding the largest deviation in the paths for the $\Delta z_t$ we can bound the largest differenct in $x_t$.  The exact solution satisfies the model equations exactly.  The error in the proposed solution provides a conservative bound on the largest change in $z$.


  \begin{gather}
    \label{eq:3}
\max_{\{x_{-},\epsilon\}} \infNorm{ \phi \eqnFunc(x_{-},g^p(x_{-},\epsilon),G^p(g^p(x_{-},\epsilon)),\epsilon) }\\
\max_{\{x_{-},\epsilon\}} \infNorm{ \phi \expct{ \eqnFunc(x_{-},g^p(x_{-},\epsilon),G^p(g^p(x_{-},\epsilon)),\epsilon)} }\\
\hat{G}^p(x,\epsilon)=G^p(x,\epsilon)+ \mathcal{I}_e(x,\epsilon)\\
\max_{\{x_{-},\epsilon\}} \infNorm{ \phi \expct{ \eqnFunc(x_{-},g^p(x_{-},\epsilon),\hat{G}^p(g^p(x_{-},\epsilon)),\epsilon)} }\\
\max_{\{x_{-},\epsilon\}} \infNorm{ \phi \expct{ \eqnFunc(x_{-},g^p(x_{-},\epsilon),B g^p(x_{-},\epsilon)+(I-F)^{-1}\psi_c,\epsilon)} }
  \end{gather}



\label{worst}
\begin{algorithm}
 \SetKwInOut{Input}{input}
 \SetKwInOut{Output}{output}
\Input{$\linMod$, $\tArg$, $\xWOarg^p$, $\XWOarg^p$, $N$}
$x_{t}=\xWOarg^p\tArg$\;
$x_{t+1}=\XWOarg^p(x_t)$\;
\For{$i=2$ to $N$}{
$x_{t+i}=\XWOarg^k(x_{t+i-1})$\;
}
\For{$i=0$ to $N-1$}{
$   z^p_{t}(x_{t-1},\epsilon_t) = H_{-1}  x^p_{t-1}(x_{t-1},\epsilon_t) + 
 H_0  x^p_{t}(x_{t-1},\epsilon_t) +  \label{defZ} 
 H_1  x^p_{t+1}(x_{t-1},\epsilon_t)$
}
\Output{$\sum_{\nu=1}^N F^{\nu-1} \phi z_{t+\nu}$}
\caption{$\fSum(\linMod, \xguss, \XWOarg^k, \ZWOarg^k, N)$} 
\label{conPath}
\end{algorithm}



\subsection{Practical Considerations for Applying the Formula}
\label{sec:practicalformula}


\paragraph{MSNTO}
 
This section describes a method for finding optima in a closed bounded domain.
Let $D=[a,b]$ in $R^s$ where $a_i \le x_i \le b_i$. We want to find  $M=f(x^\ast)= \max_{x \in D}f(x)$.
Th number theoretic (Quasi-Monte Carlo) approached is designed for finding optima with many local optima.  It is based on the used of space filling points.\cite{Xu2005}
The number theoretic method of optimization includes two basic steps.
\begin{enumerate}
\item Choose a set $p=x_i, i=1,\dots,n$ of potential optima that are uniformly scatered.
\item Find $M_n\text{ and } x^\ast \in p =\max_{1\le i \le n}f(x_i)$
\end{enumerate}

Even when choosing $p$ wisely convergence slow until \cite{Niederreiter1983}.  They speed up the convergence by contracting the domain.
Their algorithm discards all but the best in the domain. Must have very many points to guarantee choosing a point close to the optimum.
Multilocal searches.

MSNTO selects a finite number of sample points uniformly scattered on D.  Next it discard inferior points retaining small sample of potential maxima.  These form the clusters for the next step.  The algoritm performs a domain contraction on each of the clusters.






\section{An Algorithm for Improving Proposed Time Invariant Solutions}
\label{sec:algoforsoln}












\subsubsection{Algorithm Pseudo-code}
\label{sec:pseudocode}

\begin{enumerate}
\item Specify a (collection of) model equations systems(s)
For all relevant values of $\tArg$ there is one and only one solution
\item specify a linear reference model
\item specify an initial guess for decision rule $\xWargK$ obtain conditional expectations function
\item decide upon the degree of approximation for the Smolyak polynomial representation
\item decide upon the number of conditional expectations function recursive iterations.  Fewer means more truncation error. In effect imposes nonlinear constraints for number of period specified and the linear reference model thereafter.
\item algorithm uses conditional expectation and model equations to produce an improved approximation to the decision rule.
\item the algorithm finds a solution to the system that constrains $x_t= x_g$ determining $x_t,z_t$ at the interpolation points for $x_{t-1},\epsilon_t$  This can be done in parallel
\item the data from the interpolation points is used to produce an updated decision rule
\end{enumerate}



It will be convenient for describing the algorithms to
 augment the model equations to include auxiliary variables so that we can write the model in the form
$  \eqnFunc(x_{t-1},x_t,\expct{x_{t+1}},\epsilon)=0$.  We will consider time invariant model solutions $x_t=\xWarg$.  Compute $\XWOarg(x_{t-1})\equiv \expct{\xWarg}$. We will have

\begin{gather}
\eqnFunc(x_{t-1},\xWarg,\XWOarg(\xWarg),\epsilon)=0 \intertext{Given a linear reference model,}
\linMod  \equiv \linModMats \intertext{we can define functions $\zWOarg$ by}
\zWarg\equiv H
\begin{bmatrix}
x_{t-1}\\ \xWarg\\ \XWOarg(\xWarg)
\end{bmatrix}+\phi_\epsilon \epsilon_t+\phi_c\intertext{compute $\ZWOarg$ }
\ZWOarg(x_{t-1})\equiv \expct{\zWarg}\intertext{define conditional expectations paths for $x_t, z_t$}
x_{t+k+1}=\XWOarg(x_{t+k}),\,\,\,z_{t+k+1}=\ZWOarg(x_{t+k})\,\,\,\,  \forall k\ge 0\\
	 \mathcal{X}_{t} =B x_{t-1}+ \phi \psi_\epsilon\epsilon + (I - F)^{-1} \phi \psi_c + \sum_{\sForSum=0}^\infty F^s \phi z_{t+\sForSum} 
\intertext{and}
	 \mathcal{X}_{t+1} =B \mathcal{X}_{t}  + (I - F)^{-1} \phi \psi_c+ \sum_{\sForSum =0}^\infty F^\sForSum \phi z_{t+\sForSum+1} \,\,\,\,\,\forall t,k \ge  0 \\
	\expct{ \mathcal{X}_{t+1}} =B \mathcal{X}_{t}  + (I - F)^{-1} \phi \psi_c+ \sum_{\sForSum =0}^\infty F^\sForSum \phi \expct{z_{t+\sForSum+1}} \,\,\,\,\,\forall t,k \ge  0\\
	\expct{ \mathcal{X}_{t+1}} =B \mathcal{X}_{t}  + (I - F)^{-1} \phi \psi_c+ \sum_{\sForSum =0}^\infty F^\sForSum \phi \ZWOarg(x_{t+\nu}) \,\,\,\,\,\forall t,k \ge  0
\end{gather}





The $g$ function uses equation \ref{theSeries} to construct values needed to evaluate the model equations.
\begin{gather}
  g(x_{t-1},\epsilon_t,z_t)=
  \begin{bmatrix}
    x_{t-1}\\
B x_{t-1}+ \phi \psi_\epsilon\epsilon + (I - F)^{-1} \phi \psi_c + \sum_{\sForSum=0}^K F^s \phi z_{t+\sForSum} \\
B x_{t}+   (I - F)^{-1} \phi \psi_c + \sum_{\sForSum=0}^K F^s \phi z_{t+\sForSum} \\
\epsilon_t
  \end{bmatrix}
\end{gather}
\begin{algorithm}
 \SetKwInOut{Input}{input}
 \SetKwInOut{Output}{output}
\Input{$\linMod$, $\sum_{\nu=1}^N F^{\nu-1} \phi z_{t+\nu}$}
$x_t(x_{t-1},\epsilon_t,z_t)=B x_{t-1} + (I-F)^{-1} \phi. \psi_c+\phi \psi_\epsilon \epsilon_t + \phi . \psi_z z_t + F\sum_{\nu=1}^N F^{\nu-1} \phi z_{t+\nu} $\;
$\expct{x_{t+1}}(x_{t-1},\epsilon_t,z_t)=B x_{t}(x_{t-1},\epsilon_t,z_t) + (I-F)^{-1} \phi. \psi_c + \sum_{\nu=1}^N F^{\nu-1} \phi z_{t+\nu} $\;
$g(x_{t-1},\epsilon_t,z_t)\equiv
\begin{bmatrix}
  x_{t-1}\\x_t(x_{t-1},\epsilon_t,z_t)\\\expct{x_{t+1}}(x_{t-1},\epsilon_t,z_t)\\ \epsilon_t
\end{bmatrix}
$\;
\Output{$g(x_{t-1},\epsilon_t,z_t)$}
\caption{$\modArgs(\linMod,\sum_{\nu=1}^N F^{\nu-1} \phi z_{t+\nu})$}
\label{gEqn}
\end{algorithm}

We will represent model solutions using the anisotropic Smolyak interpolation method outlined in \cite{Judd2014}.\footnote{ This choice makes it possible to precompute all the integrals necessary for the conditional expectations calculations. See section \ref{sec:smolyakinterp} for details.}  This will require solving
the following system at a prespecified number of points.  This can be done in parallel. We can use the linear system associated with some 
linear reference model, $\linMod$, as the initial guess.  
It is not necessary, but it may be useful to use some linearization of the model, $\eqnFunc$.  Typically one would start with the inital values for $z_t\tArg=0$.

From the  trial value for the model solution codified in the \ADR, $\xWOarg^k\tArg$, we obtain the corresponding \ADRCE, $\XWOarg^k\tNo$ and, as
 shown in algorithm \ref{fSum}, using
a guess for $x_t\tArg$ we compute a conditional expectations path of length N from $\tArg$ forward.  We use the \ADRCE expressions for $Z^k$ to compute the expected values for future $z_t$. As shown in algorithm \ref{gEqn}, we 
will use these values in the formula \ref{theSeries} to compute $x_t$ and $\expct{x_{t+1}}$ Algorithm \ref{gEqn} provides all the arguments needed for solving the equation system, $\eqnFunc$.  Algorithm \ref{theSys} obtains  $\xWOarg^k\tArg,\zWOarg^k\tArg$ at the designated points so that the model equations are satisfied and the $\xguss=x$

\begin{algorithm}
 \SetKwInOut{Input}{input}
 \SetKwInOut{Output}{output}
\Input{$\linMod$, $\xguss$, $\XWOarg^k$, $\ZWOarg^k$, $N$}
$x_{t+1}=\XWOarg^k(\xguss)$\;
$z_{t+1}=\ZWOarg^k(\xguss)$\;
\For{$i=2$ to $N$}{
$x_{t+i}=\XWOarg^k(x_{t+i-1})$\;
$z_{t+i}=\ZWOarg^k(x_{t+i-1})$\;
}
\Output{$\sum_{\nu=1}^N F^{\nu-1} \phi z_{t+\nu}$}
\caption{$\fSum(\linMod, \xguss, \XWOarg^k, \ZWOarg^k, N)$} 
\label{fSum}
\end{algorithm}


\begin{algorithm}
 \SetKwInOut{Input}{input}
 \SetKwInOut{Output}{output}
\Input{$\eqnFunc$, $\linMod$,  $\XWOarg^k$, $\ZWOarg^k$, $N$, $x_{t-1}$, $\epsilon_t$}
Find $x_t$ and $z_t$
$\eqnFunc(\modArgs(\linMod,\fSum(\linMod,\xguss,\XWOarg^k,\ZWOarg^k)))=0$\;
$\xguss=x_t(x_{t-1},\epsilon)$\;
\Output{$\xWOarg^{k+1}(x_{t-1},\epsilon_t)$, $\zWOarg^{k+1}(x_{t-1},\epsilon_t)$}
\caption{$\xWOarg^{k+1}(x_{t-1},\epsilon_t)$, $\zWOarg^{k+1}(x_{t-1},\epsilon_t)$}
\label{theSys}
\end{algorithm}


\subsection{Function Approximation Representation}
\label{sec:funcApproxRep}

\subsubsection{General Issues}
\label{sec:generalissues}










\subsubsection{Smolyak Interpolation}
\label{sec:smolyakinterp}

\paragraph{Anisotropic}
It is possible to precompute each of the integrals. Since the solution will be linear combination of the polynomials, precomputing the polynomials means the weighted sum of the precomputed integrals will provide the integrals.
\paragraph{Precomputing Integrals}
\begin{algorithm}
  \SetKwInOut{Input}{input}
 \SetKwInOut{Output}{output}
\Input{$\aSmolPoly$, $\smolRngs$, $\distribSpec$} 
\Output{$\int \aSmolPoly \Pi (f_i(\epsilon_i)) d\epsilon_1 \ldots d\epsilon_k$}
\end{algorithm}



\begin{verbatim}


smolPolyExp[aSmolPoly_,smolRngs_?MatrixQ,distribSpec:{expctSpec:{{_Symbol,_}..}}]:=
With[{numEps=Length[distribSpec[[1]]],
polyVars=Sort[Cases[aSmolPoly,xx[_Integer]]]},
With[{theChebValSubs=Thread[polyVars->
MapThread[xformXValToCheb,{polyVars,smolRngs}]]
},
With[{numX=Length[polyVars]-numEps},
With[{intVarRes=genIntVars[numX,distribSpec]},
With[{polyEps=Drop[polyVars,numX],intEps=Drop[intVarRes[[2]],numX]},
With[{epsSubs=MapThread[#1->#2&,{polyEps,intEps}]},
With[{funcGuts=((aSmolPoly/.theChebValSubs)/.epsSubs)},
myExpectation[funcGuts,intVarRes[[3]]]]]]]]]]
\end{verbatim}


\subsection{RBC Example}
\label{sec:rbc-example}


\subsubsection{Approximating the Known Solution: $U(c) = Log(c)$ }
\label{sec:recov-known-solut}

\subsubsection{Approximating an Unknown Solution: $U(c) \ne Log(c)$ }
\label{sec:unknown-solutions}
\label{sec:regime-switch-model}

\subsubsection{Occasionally Binding Constraints}
\label{sec:obc-solut}

The algorithm we have described,
uses a proposed deterministic map
characterizing the evolution of expected values for
the dynamic system going forward. It then solves
a deterministic problem at time t to improve the proposed solution.
There is nothing in the algorithm that precludes accommodating  inequality
constraints.\footnote{See section \ref{sec:regime-switch-model} characterizing
  models with regime switching.}

\paragraph{unknown solutions occasionally binding constraints}

For example, the Euler equations for the  neoclassical growth  model 
\label{sec:simple-rbc-model-ext} can be written as
\begin{tcolorbox}[ams gather]
if(\mu>0 \land (k_t - (1-d)k_{t-1}-\upsilon I_{ss})=0)\\
  \lambda_t -\frac{1}{c_t}\\
c_t+k_t-\theta_tk_{t-1}^\alpha\\
N_t-\lambda_t \theta_t\\
\theta_t-e^{(\rho\ln(\theta_{t-1})+\epsilon)}\\
\lambda_t + \sqrt{\mu_t} - (\alpha k_t^{(\alpha-1)}\delta N_{t+1}+\lambda_{t+1} \delta (1-d)+\sqrt{\mu_{t+1}}+\delta (1-d)\\
I_t-(K_t-(1-d)k_{t-1})\\
\mu_t(k_t - (1-d) k_{t-1}-\upsilon I_{ss})\\
\end{tcolorbox}
\begin{tcolorbox}[ams gather]
if(\mu=0 \land (k_t - (1-d)k_{t-1}-\upsilon I_{ss})\ge 0)\\
  \lambda_t -\frac{1}{c_t}\\
c_t+k_t-\theta_tk_{t-1}^\alpha\\
N_t-\lambda_t \theta_t\\
\theta_t-e^{(\rho\ln(\theta_{t-1})+\epsilon)}\\
\lambda_t + \sqrt{\mu_t} - (\alpha k_t^{(\alpha-1)}\delta N_{t+1}+\lambda_{t+1} \delta (1-d)+\sqrt{\mu_{t+1}}+\delta (1-d)\\
I_t-(K_t-(1-d)k_{t-1})\\
\mu_t(k_t - (1-d) k_{t-1}-\upsilon I_{ss})
\end{tcolorbox}
\begin{verbatim}
lam[t] -1/cc[t],
cc[t] + kk[t]-((theta[t])*(kk[t-1]^alpha)),
nlPart[t] -(nlPartRHS=lam[t]*theta[t]),
theta[t]-E^(rho*Log[theta[t-1]] + eps[theta][t]),
lam[t] +mu1[t] - (alpha*kk[t]^(-1+alpha)*delta*nlPart[t+1]+
lam[t+1]*delta*(1-dd)+mu1Sqrt[t+1]*delta*(1-dd)),
II[t] -(kk[t]-(1-dd)*kk[t-1]),
mu1[t]*(kk[t]-(1-dd)*kk[t-1]-upsilon*IIss)
}
rbcEqnsNotBinding={
lam[t] -1/cc[t],
cc[t] + kk[t]-((theta[t])*(kk[t-1]^alpha)),
nlPart[t] -(nlPartRHS=lam[t]*theta[t]),
theta[t]-E^(rho*Log[theta[t-1]] + eps[theta][t]),
lam[t] +mu1[t] - (alpha*kk[t]^(-1+alpha)*delta*nlPart[t+1]+lam[t+1]*delta*(1-dd)+mu1[t+1]*delta*(1-dd)),
II[t] -(kk[t]-(1-dd)*kk[t-1]),
mu1[t]*(kk[t]-(1-dd)*kk[t-1]-upsilon*IIss)
}
\end{verbatim}



\section{Future Work}
\label{sec:future}
\section{Conclusions}
\label{sec:conc}




\bibliographystyle{plainnat}
\bibliography{anderson,files}
\appendix






\end{document}

